{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import seaborn as sns; sns.set()\n",
    "%matplotlib inline\n",
    "\n",
    "import keras\n",
    "from keras import objectives\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Input, Multiply, Add\n",
    "from keras.optimizers import Adam, Nadam\n",
    "import salty\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from random import shuffle\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "#Keras build\n",
    "from keras import backend as K\n",
    "from keras.objectives import binary_crossentropy #objs or losses\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Lambda, Layer\n",
    "from keras.layers.core import Dense, Activation, Flatten, RepeatVector\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.layers.convolutional import Convolution1D\n",
    "\n",
    "#cation data\n",
    "cations = pd.read_csv('../data/cations.csv')\n",
    "cations = cations['smiles_string']\n",
    "salts = pd.read_csv('../data/salts.csv')\n",
    "salts = salts['smiles_string']\n",
    "categories = pd.read_csv('../data/categories.csv')\n",
    "categories = categories['category']\n",
    "coldic = pd.read_csv('../data/coldic.csv')\n",
    "coldic = coldic.to_dict(orient='records')[0]\n",
    "salt_coldic = pd.read_csv('../data/salt_coldic.csv')\n",
    "salt_coldic = salt_coldic.to_dict(orient='records')[0]\n",
    "salt_categories = pd.read_csv('../data/salt_categories.csv')\n",
    "salt_categories = salt_categories['category']\n",
    "density_coldic = pd.read_csv('../data/density_coldic.csv')\n",
    "density_coldic = density_coldic.to_dict(orient='records')[0]\n",
    "density_categories = pd.read_csv('../data/density_categories.csv')\n",
    "density_categories = density_categories['category']\n",
    "\n",
    "#supporting functions\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "from scripts import *\n",
    "\n",
    "#training array info\n",
    "smile_max_length = 105\n",
    "import json\n",
    "f = open(\"../data/salt_char_to_index.json\",\"r\")\n",
    "ani_char_to_index = json.loads(f.read())\n",
    "ani_char_set = set(ani_char_to_index.keys())\n",
    "ani_char_list = list(ani_char_to_index.keys())\n",
    "ani_chars_in_dict = len(ani_char_list)\n",
    "ani_index_to_char = dict((i, c) for i, c in enumerate(ani_char_list))\n",
    "\n",
    "#training array info\n",
    "import json\n",
    "f = open(\"../data/gdb_char_to_index.json\",\"r\")\n",
    "cat_char_to_index = json.loads(f.read())\n",
    "cat_char_set = set(cat_char_to_index.keys())\n",
    "cat_char_list = list(cat_char_to_index.keys())\n",
    "cat_chars_in_dict = len(cat_char_list)\n",
    "cat_index_to_char = dict((i, c) for i, c in enumerate(cat_char_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.topology.InputLayer at 0x7f732db704e0>,\n",
       " <keras.layers.convolutional.Conv1D at 0x7f732daaffd0>,\n",
       " <keras.layers.convolutional.Conv1D at 0x7f732db70d68>,\n",
       " <keras.layers.convolutional.Conv1D at 0x7f732da68710>,\n",
       " <keras.layers.core.Flatten at 0x7f732da68b38>,\n",
       " <keras.layers.core.Dense at 0x7f732d9d5eb8>,\n",
       " <keras.layers.core.Dense at 0x7f732d980860>,\n",
       " <keras.layers.core.Dense at 0x7f732d980f60>,\n",
       " <keras.layers.core.Lambda at 0x7f732d9ae4a8>,\n",
       " <keras.layers.core.Dense at 0x7f732d9aecc0>,\n",
       " <keras.layers.core.RepeatVector at 0x7f732d958f60>,\n",
       " <keras.layers.recurrent.GRU at 0x7f732d944710>,\n",
       " <keras.layers.recurrent.GRU at 0x7f732d9243c8>,\n",
       " <keras.layers.recurrent.GRU at 0x7f732d7a1470>,\n",
       " <keras.layers.wrappers.TimeDistributed at 0x7f732d50e860>]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anionvae = MoleculeVAE()\n",
    "anionvae.create(char_set, max_length=62, \n",
    "                weights_file='../models/1mil_GDB17_62smi_saltchar_500k_mix_anion_5.h5')\n",
    "anionvae.autoencoder.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.topology.InputLayer at 0x7f72e7131f98>,\n",
       " <keras.layers.convolutional.Conv1D at 0x7f72e70e1828>,\n",
       " <keras.layers.convolutional.Conv1D at 0x7f72e7197710>,\n",
       " <keras.layers.convolutional.Conv1D at 0x7f72e70feb38>,\n",
       " <keras.layers.core.Flatten at 0x7f72e70a8dd8>,\n",
       " <keras.layers.core.Dense at 0x7f72e7056fd0>,\n",
       " <keras.layers.core.Dense at 0x7f72e7017ef0>,\n",
       " <keras.layers.core.Dense at 0x7f72e7031b70>,\n",
       " <keras.layers.core.Lambda at 0x7f72e6fc6e10>,\n",
       " <keras.layers.core.Dense at 0x7f72e6fc6e80>,\n",
       " <keras.layers.core.RepeatVector at 0x7f72e6f8d9e8>,\n",
       " <keras.layers.recurrent.GRU at 0x7f72e6ff7160>,\n",
       " <keras.layers.recurrent.GRU at 0x7f72e6fbbda0>,\n",
       " <keras.layers.recurrent.GRU at 0x7f72e6eb84e0>,\n",
       " <keras.layers.wrappers.TimeDistributed at 0x7f72e6bb59b0>]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cationvae = MoleculeVAE()\n",
    "cationvae.create(cat_char_set, max_length=51, \n",
    "                 weights_file='../models/1mil_GDB17_500K_mix_500K_cation_5.h5')\n",
    "cationvae.autoencoder.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "cationvae.save('cation_vae.h5')\n",
    "anionvae.save('anion_vae.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "saltvae = TwoMoleculeVAE()\n",
    "saltvae.create(cat_char_set, ani_char_set, cat_weights_file='cation_vae.h5',\n",
    "               ani_weights_file='anion_vae.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed:\tCCCCCCC[N+](C)(C)CC\n",
      "sample:\tCCCCCCC[N+](C)(C)C)1(NCF54 s#13NN5O#3 H78HnN73CO82S\n"
     ]
    }
   ],
   "source": [
    "seed = salts[random.randint(0,len(salts))].split('.')[0]\n",
    "print(\"seed:\\t{}\".format(seed))\n",
    "string = \"\"\n",
    "\n",
    "for i in saltvae.cation_autoencoder.predict(one_hot(seed, cat_char_to_index, \n",
    "                                               smile_max_length=51))[:1]:\n",
    "    if len(i.shape) > 2:\n",
    "        i = i[0] #for qspr chemvae there is an extra dim\n",
    "    for j in i:\n",
    "#         print(j.shape)\n",
    "        index = sample(j, temperature=0.2)\n",
    "        string += cat_index_to_char[index]\n",
    "print(\"sample:\\t{}\".format(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed:\tc1c(cn[n-]1)[N+](=O)[O-]\n",
      "sample:\tc1c(cn[n-]1)[N+](=O)[O-]                                      \n"
     ]
    }
   ],
   "source": [
    "seed = salts[random.randint(0,len(salts))].split('.')[1]\n",
    "print(\"seed:\\t{}\".format(seed))\n",
    "string = \"\"\n",
    "\n",
    "for i in saltvae.anion_autoencoder.predict(one_hot(seed, ani_char_to_index, \n",
    "                                               smile_max_length=62))[:1]:\n",
    "    if len(i.shape) > 2:\n",
    "        i = i[0] #for qspr chemvae there is an extra dim\n",
    "    for j in i:\n",
    "#         print(j.shape)\n",
    "        index = sample(j, temperature=0.5)\n",
    "        string += ani_index_to_char[index]\n",
    "print(\"sample:\\t{}\".format(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "##plot\n",
    "#import matplotlib.pylab as plt\n",
    "#import seaborn as sns; sns.set()\n",
    "#from matplotlib import colors\n",
    "#from itertools import cycle\n",
    "\n",
    "#data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import json\n",
    "import random\n",
    "import copy\n",
    "\n",
    "#ML\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras import objectives\n",
    "from keras.objectives import binary_crossentropy #objs or losses\n",
    "from keras.layers import Dense, Dropout, Input, Multiply, Add, Lambda\n",
    "from keras.layers.core import Dense, Activation, Flatten, RepeatVector\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.layers.convolutional import Convolution1D\n",
    "\n",
    "#chem\n",
    "#import salty\n",
    "#from rdkit import Chem\n",
    "#from rdkit.Chem.Fingerprints import FingerprintMols\n",
    "#from rdkit import DataStructs\n",
    "#from rdkit.Chem import Draw\n",
    "\n",
    "\n",
    "class TwoMoleculeVAE():\n",
    "\n",
    "    cation_autoencoder = None\n",
    "    anion_autoencoder = None\n",
    "    \n",
    "    def create(self,\n",
    "               cat_charset,\n",
    "               ani_charset,\n",
    "               cat_max_length = 51,\n",
    "               cat_weights_file = None,\n",
    "               ani_max_length = 62,\n",
    "               ani_weights_file = None,\n",
    "               latent_rep_size = 292,\n",
    "               qspr_weights_file = None,\n",
    "               qspr = False):\n",
    "        \n",
    "        ### cation\n",
    "        max_length = cat_max_length\n",
    "        charset = cat_charset\n",
    "        charset_length = len(charset)\n",
    "        x = Input(shape=(max_length, charset_length))\n",
    "        _, z = self._buildEncoder(x, latent_rep_size, max_length)\n",
    "        self.cation_encoder = Model(x, z)\n",
    "#         for layer in self.cation_encoder.layers:\n",
    "#             layer.name = 'cation_' + layer.name\n",
    "            \n",
    "        encoded_input = Input(shape=(latent_rep_size,))\n",
    "        self.cation_decoder = Model(\n",
    "            encoded_input,\n",
    "            self._buildDecoder(\n",
    "                encoded_input,\n",
    "                latent_rep_size,\n",
    "                max_length,\n",
    "                charset_length\n",
    "            )\n",
    "        )\n",
    "#         for layer in self.cation_decoder.layers:\n",
    "#             layer.name = 'cation_' + layer.name\n",
    "\n",
    "        x1 = Input(shape=(max_length, charset_length))\n",
    "        vae_loss, z1 = self._buildEncoder(x1, latent_rep_size, max_length)\n",
    "        if qspr:\n",
    "            self.cation_autoencoder = Model(\n",
    "                x1,\n",
    "                self._buildDecoderQSPR(\n",
    "                    z1,\n",
    "                    latent_rep_size,\n",
    "                    max_length,\n",
    "                    charset_length\n",
    "                )\n",
    "            )\n",
    "#         for layer in self.cation_autoencoder.layers:\n",
    "#             layer.name = 'cation_' + layer.name\n",
    "        else:\n",
    "            self.cation_autoencoder = Model(\n",
    "                x1,\n",
    "                self._buildDecoder(\n",
    "                    z1,\n",
    "                    latent_rep_size,\n",
    "                    max_length,\n",
    "                    charset_length\n",
    "                )\n",
    "            )\n",
    "#         for layer in self.cation_autoencoder.layers:\n",
    "#             layer.name = 'cation_' + layer.name\n",
    "            \n",
    "        ### anion\n",
    "        max_length = ani_max_length\n",
    "        charset = ani_charset\n",
    "        charset_length = len(ani_charset)\n",
    "        x = Input(shape=(max_length, charset_length))\n",
    "        _, z = self._buildEncoder(x, latent_rep_size, max_length)\n",
    "        self.anion_encoder = Model(x, z)\n",
    "#         for layer in self.anion_encoder.layers:\n",
    "#             layer.name = 'anion_' + layer.name\n",
    "            \n",
    "        encoded_input = Input(shape=(latent_rep_size,))\n",
    "        self.anion_decoder = Model(\n",
    "            encoded_input,\n",
    "            self._buildDecoder(\n",
    "                encoded_input,\n",
    "                latent_rep_size,\n",
    "                max_length,\n",
    "                charset_length\n",
    "            )\n",
    "        )\n",
    "#         for layer in self.anion_decoder.layers:\n",
    "#             layer.name = 'anion_' + layer.name\n",
    "\n",
    "        x1 = Input(shape=(max_length, charset_length))\n",
    "        vae_loss, z1 = self._buildEncoder(x1, latent_rep_size, max_length)\n",
    "        if qspr:\n",
    "            self.anion_autoencoder = Model(\n",
    "                x1,\n",
    "                self._buildDecoderQSPR(\n",
    "                    z1,\n",
    "                    latent_rep_size,\n",
    "                    max_length,\n",
    "                    charset_length\n",
    "                )\n",
    "            )\n",
    "#         for layer in self.anion_autoencoder.layers:\n",
    "#             layer.name = 'anion_' + layer.name\n",
    "        else:\n",
    "            self.anion_autoencoder = Model(\n",
    "                x1,\n",
    "                self._buildDecoder(\n",
    "                    z1,\n",
    "                    latent_rep_size,\n",
    "                    max_length,\n",
    "                    charset_length\n",
    "                )\n",
    "            )\n",
    "#         for layer in self.anion_autoencoder.layers:\n",
    "#             layer.name = 'anion_' + layer.name\n",
    "            \n",
    "        self.qspr = Model(\n",
    "            x1,\n",
    "            self._buildQSPR(\n",
    "                z1,\n",
    "                latent_rep_size,\n",
    "                max_length,\n",
    "                charset_length\n",
    "            )\n",
    "        )\n",
    "        \n",
    "\n",
    "        if cat_weights_file:\n",
    "            self.cation_autoencoder.load_weights(cat_weights_file, by_name = True)\n",
    "            self.cation_encoder.load_weights(cat_weights_file, by_name = True)\n",
    "            self.cation_decoder.load_weights(cat_weights_file, by_name = True)\n",
    "        if ani_weights_file:\n",
    "            self.anion_autoencoder.load_weights(ani_weights_file, by_name = True)\n",
    "            self.anion_encoder.load_weights(ani_weights_file, by_name = True)\n",
    "            self.anion_decoder.load_weights(ani_weights_file, by_name = True)\n",
    "#         if qspr_weights_file:\n",
    "#             self.qspr.load_weights(weights_file, by_name = True)\n",
    "        if qspr:\n",
    "            self.autoencoder.compile(optimizer = 'Adam',\n",
    "                                     loss = {'decoded_mean': vae_loss, 'qspr': 'mean_squared_error'},\n",
    "                                     metrics = ['accuracy', 'mse'])\n",
    "        else:\n",
    "            self.cation_autoencoder.compile(optimizer = 'Adam',\n",
    "                                     loss = {'decoded_mean': vae_loss},\n",
    "                                     metrics = ['accuracy'])\n",
    "            self.anion_autoencoder.compile(optimizer = 'Adam',\n",
    "                                     loss = {'decoded_mean': vae_loss},\n",
    "                                     metrics = ['accuracy'])\n",
    "    def _buildEncoder(self, x, latent_rep_size, max_length, epsilon_std = 0.01):\n",
    "        h = Convolution1D(9, 9, activation = 'relu', name='conv_1')(x)\n",
    "        h = Convolution1D(9, 9, activation = 'relu', name='conv_2')(h)\n",
    "        h = Convolution1D(10, 11, activation = 'relu', name='conv_3')(h)\n",
    "        h = Flatten(name='flatten_1')(h)\n",
    "        h = Dense(435, activation = 'relu', name='dense_1')(h)\n",
    "\n",
    "        def sampling(args):\n",
    "            z_mean_, z_log_var_ = args\n",
    "            batch_size = K.shape(z_mean_)[0]\n",
    "            epsilon = K.random_normal(shape=(batch_size, latent_rep_size), mean=0., stddev = epsilon_std)\n",
    "            return z_mean_ + K.exp(z_log_var_ / 2) * epsilon\n",
    "\n",
    "        z_mean = Dense(latent_rep_size, name='z_mean', activation = 'linear')(h)\n",
    "        z_log_var = Dense(latent_rep_size, name='z_log_var', activation = 'linear')(h)\n",
    "\n",
    "        def vae_loss(x, x_decoded_mean):\n",
    "            x = K.flatten(x)\n",
    "            x_decoded_mean = K.flatten(x_decoded_mean)\n",
    "            xent_loss = max_length * objectives.binary_crossentropy(x, x_decoded_mean)\n",
    "            kl_loss = - 0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis = -1)\n",
    "            return xent_loss + kl_loss\n",
    "\n",
    "        return (vae_loss, Lambda(sampling, output_shape=(latent_rep_size,), name='lambda')([z_mean, z_log_var]))\n",
    "\n",
    "    def _buildDecoderQSPR(self, cat_z, ani_z, latent_rep_size, max_length, charset_length):\n",
    "\n",
    "        h = Dense(latent_rep_size, name='cation_latent_input', activation = 'relu')(cat_z)\n",
    "        h = RepeatVector(max_length, name='cation_repeat_vector')(h)\n",
    "        h = GRU(501, return_sequences = True, name='cation_gru_1')(h)\n",
    "        h = GRU(501, return_sequences = True, name='cation_gru_2')(h)\n",
    "        h = GRU(501, return_sequences = True, name='cation_gru_3')(h)\n",
    "        cat_smiles_decoded = TimeDistributed(Dense(charset_length, activation='softmax'), name='cation_decoded_mean')(h)\n",
    "        \n",
    "        h = Dense(latent_rep_size, name='anion_latent_input', activation = 'relu')(ani_z)\n",
    "        h = RepeatVector(max_length, name='anion_repeat_vector')(h)\n",
    "        h = GRU(501, return_sequences = True, name='anion_gru_1')(h)\n",
    "        h = GRU(501, return_sequences = True, name='anion_gru_2')(h)\n",
    "        h = GRU(501, return_sequences = True, name='anion_gru_3')(h)\n",
    "        ani_smiles_decoded = TimeDistributed(Dense(charset_length, activation='softmax'), name='anion_decoded_mean')(h)\n",
    "\n",
    "        h = Dense(latent_rep_size, name='qspr_input', activation='relu')([cat_z, ani_z])\n",
    "        h = Dense(100, activation='relu', name='hl_1')(h)\n",
    "        h = Dropout(0.5)(h)\n",
    "        smiles_qspr = Dense(1, activation='linear', name='qspr')(h)\n",
    "\n",
    "        return smiles_decoded, smiles_qspr\n",
    "\n",
    "    def _buildDecoder(self, z, latent_rep_size, max_length, charset_length):\n",
    "\n",
    "        h = Dense(latent_rep_size, name='latent_input', activation = 'relu')(z)\n",
    "        h = RepeatVector(max_length, name='repeat_vector')(h)\n",
    "        h = GRU(501, return_sequences = True, name='gru_1')(h)\n",
    "        h = GRU(501, return_sequences = True, name='gru_2')(h)\n",
    "        h = GRU(501, return_sequences = True, name='gru_3')(h)\n",
    "        smiles_decoded = TimeDistributed(Dense(charset_length, activation='softmax'), name='decoded_mean')(h)\n",
    "\n",
    "        return smiles_decoded\n",
    "    \n",
    "    def _buildQSPR(self, z, latent_rep_size, max_length, charset_length):\n",
    "        h = Dense(latent_rep_size, name='latent_input', activation='relu')(z)\n",
    "        h = Dense(100, activation='relu', name='hl_1')(h)\n",
    "        h = Dropout(0.5)(h)\n",
    "        return Dense(1, activation='linear', name='qspr')(h)\n",
    "\n",
    "    def save(self, filename):\n",
    "        self.autoencoder.save_weights(filename)\n",
    "    \n",
    "    def load(self, charset, weights_file, latent_rep_size = 292):\n",
    "        self.create(charset, weights_file = weights_file, latent_rep_size = latent_rep_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
