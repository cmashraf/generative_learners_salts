{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##plot\n",
    "# from IPython.display import clear_output, display\n",
    "# import matplotlib.pylab as plt\n",
    "# import seaborn as sns; sns.set()\n",
    "# from matplotlib import colors\n",
    "# from itertools import cycle\n",
    "\n",
    "# #chem\n",
    "# import salty\n",
    "# import gains as genetic\n",
    "# from rdkit import Chem\n",
    "# from rdkit.Chem.Fingerprints import FingerprintMols\n",
    "# from rdkit import DataStructs\n",
    "# from rdkit.Chem import Draw\n",
    "# from rdkit.ML.Descriptors.MoleculeDescriptors import\\\n",
    "#     MolecularDescriptorCalculator as calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wesleybeckner/anaconda3/envs/py36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import seaborn as sns; sns.set()\n",
    "%matplotlib inline\n",
    "\n",
    "import keras\n",
    "from keras import objectives\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Input, Multiply, Add\n",
    "from keras.optimizers import Adam, Nadam\n",
    "import salty\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from random import shuffle\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "#Keras build\n",
    "from keras import backend as K\n",
    "from keras.objectives import binary_crossentropy #objs or losses\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Lambda, Layer\n",
    "from keras.layers.core import Dense, Activation, Flatten, RepeatVector\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.layers.convolutional import Convolution1D\n",
    "\n",
    "#cation data\n",
    "cations = pd.read_csv('../data/cations.csv')\n",
    "cations = cations['smiles_string']\n",
    "salts = pd.read_csv('../data/salts.csv')\n",
    "salts = salts['smiles_string']\n",
    "categories = pd.read_csv('../data/categories.csv')\n",
    "categories = categories['category']\n",
    "coldic = pd.read_csv('../data/coldic.csv')\n",
    "coldic = coldic.to_dict(orient='records')[0]\n",
    "salt_coldic = pd.read_csv('../data/salt_coldic.csv')\n",
    "salt_coldic = salt_coldic.to_dict(orient='records')[0]\n",
    "salt_categories = pd.read_csv('../data/salt_categories.csv')\n",
    "salt_categories = salt_categories['category']\n",
    "density_coldic = pd.read_csv('../data/density_coldic.csv')\n",
    "density_coldic = density_coldic.to_dict(orient='records')[0]\n",
    "density_categories = pd.read_csv('../data/density_categories.csv')\n",
    "density_categories = density_categories['category']\n",
    "\n",
    "#supporting functions\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "from scripts import *\n",
    "\n",
    "#training array info\n",
    "smile_max_length = 105\n",
    "import json\n",
    "f = open(\"../data/salt_char_to_index.json\",\"r\")\n",
    "char_to_index = json.loads(f.read())\n",
    "char_set = set(char_to_index.keys())\n",
    "char_list = list(char_to_index.keys())\n",
    "chars_in_dict = len(char_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcharset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m105\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_rep_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m292\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqspr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgru_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv_filters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv_kernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgru_output_units\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m501\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m <no docstring>\n",
       "\u001b[0;31mFile:\u001b[0m      /media/wesleybeckner/weshhd/wes/Dropbox/Python/py3/generative_learners_salts/notebooks/<ipython-input-2-837442963518>\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vae = MoleculeVAE()\n",
    "vae.create?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 9 9 14 925 338\n"
     ]
    }
   ],
   "source": [
    "convs = random.randint(3,10) #3 original\n",
    "grus = random.randint(3,10) #3 original\n",
    "conv_filters = random.randint(9,12) #9 original\n",
    "conv_kernel_size = random.randint(9,15) #9 original\n",
    "gru_output_units = random.randint(450,1000) #501 original\n",
    "latent_rep_size = random.randint(250,600) #292 original\n",
    "{'convolutions': convs, 'GRUs': grus, 'conv_filter_size': conv_filters,\n",
    " 'conv_kernel_size': conv_kernel_size, 'gru_output_units': gru_output_units, \n",
    " 'latent_rep_size': latent_rep_size}\n",
    "print(convs, grus, conv_filters, conv_kernel_size, gru_output_units, latent_rep_size)\n",
    "vae.create(char_set, \n",
    "           max_length=105, \n",
    "           latent_rep_size=latent_rep_size, \n",
    "           weights_file=None,\n",
    "           qspr=False,\n",
    "           conv_layers=convs,\n",
    "           gru_layers=grus,\n",
    "           conv_filters=conv_filters,\n",
    "           conv_kernel_size=conv_kernel_size,\n",
    "           gru_output_units=gru_output_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def model_search(iterations=10, data_size=100000):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    iterations: int, default 10\n",
    "        number of iterations for random search\n",
    "    datasize: int, default 100,000\n",
    "        epoch training size, max 1M when reading\n",
    "        from GDB17.1000000\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    vae: keras model\n",
    "    history: keras training history\n",
    "    \"\"\"b\n",
    "    for it in range(iterations):\n",
    "        convs = random.randint(3,10) #3 original\n",
    "        grus = random.randint(3,10) #3 original\n",
    "        conv_filters = random.randint(9,12) #9 original\n",
    "        conv_kernel_size = random.randint(9,15) #9 original\n",
    "        gru_output_units = random.randint(450,1000) #501 original\n",
    "        latent_rep_size = random.randint(250,600) #292 original\n",
    "        \n",
    "        di = {'convolutions': convs, 'GRUs': grus, 'conv_filter_size': conv_filters,\n",
    "         'conv_kernel_size': conv_kernel_size, 'gru_output_units': gru_output_units, \n",
    "         'latent_rep_size': latent_rep_size}\n",
    "        print(di)\n",
    "        \n",
    "\n",
    "        f = open(\"../data/salt_char_to_index.json\",\"r\")\n",
    "        char_to_index = json.loads(f.read())\n",
    "        char_set = set(char_to_index.keys())\n",
    "        char_list = list(char_to_index.keys())\n",
    "        chars_in_dict = len(char_list)\n",
    "        \n",
    "        vae = MoleculeVAE()\n",
    "        vae.create(char_set, \n",
    "           max_length=105, \n",
    "           latent_rep_size=latent_rep_size, \n",
    "           weights_file=None,\n",
    "           qspr=False,\n",
    "           conv_layers=convs,\n",
    "           gru_layers=grus,\n",
    "           conv_filters=conv_filters,\n",
    "           conv_kernel_size=conv_kernel_size,\n",
    "           gru_output_units=gru_output_units)\n",
    "        \n",
    "        df = pd.read_csv('../data/GDB/GDB17.1000000', names=['smiles'])\n",
    "        histories = []\n",
    "        for p in range(0,5):\n",
    "            selected = df['smiles'][data_size*p:data_size*(p+1)].reset_index(drop=True)\n",
    "            sel2 = selected.copy()\n",
    "            random.shuffle(sel2)\n",
    "            values = pd.Series(selected.values + '.' + sel2.values)\n",
    "            print(len(values))\n",
    "            padded_smiles =  [pad_smiles(i, smile_max_length) for i in values if pad_smiles(i, smile_max_length)]\n",
    "            X_train = np.zeros((data_size, smile_max_length, chars_in_dict), dtype=np.float32)\n",
    "\n",
    "            for i, smile in enumerate(padded_smiles):\n",
    "                for j, char in enumerate(smile):\n",
    "                    X_train[i, j, char_to_index[char]] = 1\n",
    "            X_train, X_test = train_test_split(X_train, test_size=0.01, random_state=42) \n",
    "            history = vae.autoencoder.fit(X_train, X_train, shuffle = False, validation_data=(X_test, X_test))\n",
    "            histories.append(history.history)\n",
    "    \n",
    "        vae.save('../models/{}.h5'.format(di))\n",
    "        with open('../models/history_{}.json'.format(di), 'w') as f:\n",
    "            json.dump(histories, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
