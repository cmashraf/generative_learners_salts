{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wesley/anaconda3/envs/py36/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import seaborn as sns; sns.set()\n",
    "%matplotlib inline\n",
    "\n",
    "import keras\n",
    "from keras import objectives\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Input, Multiply, Add\n",
    "from keras.optimizers import Adam, Nadam\n",
    "import salty\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from random import shuffle\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "#Keras build\n",
    "from keras import backend as K\n",
    "from keras.objectives import binary_crossentropy #objs or losses\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Lambda, Layer\n",
    "from keras.layers.core import Dense, Activation, Flatten, RepeatVector\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.layers.convolutional import Convolution1D\n",
    "\n",
    "#cation data\n",
    "cations = pd.read_csv('../data/cations.csv')\n",
    "cations = cations['smiles_string']\n",
    "salts = pd.read_csv('../data/salts.csv')\n",
    "salts = salts['smiles_string']\n",
    "categories = pd.read_csv('../data/categories.csv')\n",
    "categories = categories['category']\n",
    "coldic = pd.read_csv('../data/coldic.csv')\n",
    "coldic = coldic.to_dict(orient='records')[0]\n",
    "salt_coldic = pd.read_csv('../data/salt_coldic.csv')\n",
    "salt_coldic = salt_coldic.to_dict(orient='records')[0]\n",
    "salt_categories = pd.read_csv('../data/salt_categories.csv')\n",
    "salt_categories = salt_categories['category']\n",
    "density_coldic = pd.read_csv('../data/density_coldic.csv')\n",
    "density_coldic = density_coldic.to_dict(orient='records')[0]\n",
    "density_categories = pd.read_csv('../data/density_categories.csv')\n",
    "density_categories = density_categories['category']\n",
    "\n",
    "#supporting functions\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "from scripts import *\n",
    "\n",
    "#training array info\n",
    "smile_max_length = 105\n",
    "import json\n",
    "f = open(\"../data/salt_char_to_index.json\",\"r\")\n",
    "char_to_index = json.loads(f.read())\n",
    "char_set = set(char_to_index.keys())\n",
    "char_list = list(char_to_index.keys())\n",
    "chars_in_dict = len(char_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns; sns.set()\n",
    "from matplotlib import colors\n",
    "from itertools import cycle\n",
    "\n",
    "#data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import json\n",
    "import random\n",
    "import copy\n",
    "\n",
    "#ML\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras import objectives\n",
    "from keras.objectives import binary_crossentropy #objs or losses\n",
    "from keras.layers import Dense, Dropout, Input, Multiply, Add, Lambda\n",
    "from keras.layers.core import Dense, Activation, Flatten, RepeatVector\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.layers.convolutional import Convolution1D\n",
    "\n",
    "#chem\n",
    "import salty\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.Fingerprints import FingerprintMols\n",
    "from rdkit import DataStructs\n",
    "from rdkit.Chem import Draw\n",
    "\n",
    "\n",
    "class MoleculeVAE():\n",
    "\n",
    "    autoencoder = None\n",
    "    \n",
    "    def create(self,\n",
    "               charset,\n",
    "               max_length = 105,\n",
    "               latent_rep_size = 292,\n",
    "               weights_file = None,\n",
    "               qspr = False,\n",
    "               conv_layers=3,\n",
    "               gru_layers=3,\n",
    "               conv_filters=3, \n",
    "               conv_kernel_size=9,\n",
    "               gru_output_units=501):\n",
    "        charset_length = len(charset)\n",
    "    \n",
    "        x = Input(shape=(max_length, charset_length))\n",
    "        _, z = self._buildEncoder(x, \n",
    "                                  latent_rep_size, \n",
    "                                  max_length,\n",
    "                                  conv_layers, \n",
    "                                  conv_filters, \n",
    "                                  conv_kernel_size)\n",
    "        self.encoder = Model(x, z)\n",
    "        encoded_input = Input(shape=(latent_rep_size,))\n",
    "        self.decoder = Model(\n",
    "            encoded_input,\n",
    "            self._buildDecoder(\n",
    "                encoded_input,\n",
    "                latent_rep_size,\n",
    "                max_length,\n",
    "                charset_length,\n",
    "                gru_layers,\n",
    "                gru_output_units\n",
    "            )\n",
    "        )\n",
    "\n",
    "        x1 = Input(shape=(max_length, charset_length))\n",
    "        vae_loss, z1 = self._buildEncoder(x1, \n",
    "                                          latent_rep_size, \n",
    "                                          max_length,\n",
    "                                          conv_layers, \n",
    "                                          conv_filters,\n",
    "                                          conv_kernel_size)\n",
    "        \n",
    "        if qspr:\n",
    "            self.autoencoder = Model(\n",
    "                x1,\n",
    "                self._buildDecoderQSPR(\n",
    "                    z1,\n",
    "                    latent_rep_size,\n",
    "                    max_length,\n",
    "                    charset_length,\n",
    "                    gru_layers,\n",
    "                    gru_output_units\n",
    "                )\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            self.autoencoder = Model(\n",
    "                x1,\n",
    "                self._buildDecoder(\n",
    "                    z1,\n",
    "                    latent_rep_size,\n",
    "                    max_length,\n",
    "                    charset_length,\n",
    "                    gru_layers,\n",
    "                    gru_output_units\n",
    "                )\n",
    "            )\n",
    "            \n",
    "        self.qspr = Model(\n",
    "            x1,\n",
    "            self._buildQSPR(\n",
    "                z1,\n",
    "                latent_rep_size,\n",
    "                max_length,\n",
    "                charset_length\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "        if weights_file:\n",
    "            self.autoencoder.load_weights(weights_file, by_name = True)\n",
    "            self.encoder.load_weights(weights_file, by_name = True)\n",
    "            self.decoder.load_weights(weights_file, by_name = True)\n",
    "            self.qspr.load_weights(weights_file, by_name = True)\n",
    "        if qspr:\n",
    "            self.autoencoder.compile(optimizer = 'Adam',\n",
    "                                     loss = {'decoded_mean': vae_loss, 'qspr': 'mean_squared_error'},\n",
    "                                     metrics = ['accuracy', 'mse'])\n",
    "        else:\n",
    "            self.autoencoder.compile(optimizer = 'Adam',\n",
    "                                     loss = {'decoded_mean': vae_loss},\n",
    "                                     metrics = ['accuracy'])\n",
    "    def _buildEncoder(self, x, latent_rep_size, max_length, epsilon_std = 0.01, conv_layers=3, conv_filters=9, conv_kernel_size=9):\n",
    "        h = Convolution1D(9, 9, activation = 'relu', name='conv_1')(x)\n",
    "        for convolution in range(conv_layers-2):\n",
    "            h = Convolution1D(conv_filters, conv_kernel_size, activation = 'relu', name='conv_{}'.format(convolution+2))(h)\n",
    "        h = Convolution1D(10, 11, activation = 'relu', name='conv_{}'.format(conv_layers))(h)\n",
    "        h = Flatten(name='flatten_1')(h)\n",
    "        h = Dense(435, activation = 'relu', name='dense_1')(h)\n",
    "\n",
    "        def sampling(args):\n",
    "            z_mean_, z_log_var_ = args\n",
    "            batch_size = K.shape(z_mean_)[0]\n",
    "            epsilon = K.random_normal(shape=(batch_size, latent_rep_size), mean=0., stddev = epsilon_std)\n",
    "            return z_mean_ + K.exp(z_log_var_ / 2) * epsilon\n",
    "\n",
    "        z_mean = Dense(latent_rep_size, name='z_mean', activation = 'linear')(h)\n",
    "        z_log_var = Dense(latent_rep_size, name='z_log_var', activation = 'linear')(h)\n",
    "\n",
    "        def vae_loss(x, x_decoded_mean):\n",
    "            x = K.flatten(x)\n",
    "            x_decoded_mean = K.flatten(x_decoded_mean)\n",
    "            xent_loss = max_length * objectives.binary_crossentropy(x, x_decoded_mean)\n",
    "            kl_loss = - 0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis = -1)\n",
    "            return xent_loss + kl_loss\n",
    "\n",
    "        return (vae_loss, Lambda(sampling, output_shape=(latent_rep_size,), name='lambda')([z_mean, z_log_var]))\n",
    "\n",
    "    def _buildDecoderQSPR(self, z, latent_rep_size, max_length, charset_length, gru_layers=3):\n",
    "\n",
    "        h = Dense(latent_rep_size, name='latent_input', activation = 'relu')(z)\n",
    "        h = RepeatVector(max_length, name='repeat_vector')(h)\n",
    "        h = GRU(501, return_sequences = True, name='gru_1')(h)\n",
    "        for gru in range(gru_layers-2):\n",
    "            h = GRU(501, return_sequences = True, name='gru_{}'.format(gru+2))(h)\n",
    "        h = GRU(501, return_sequences = True, name='gru_{}'.format(gru_layers))(h)\n",
    "        smiles_decoded = TimeDistributed(Dense(charset_length, activation='softmax'), name='decoded_mean')(h)\n",
    "\n",
    "        h = Dense(latent_rep_size, name='qspr_input', activation='relu')(z)\n",
    "        h = Dense(100, activation='relu', name='hl_1')(h)\n",
    "        h = Dropout(0.5)(h)\n",
    "        smiles_qspr = Dense(1, activation='linear', name='qspr')(h)\n",
    "\n",
    "        return smiles_decoded, smiles_qspr\n",
    "\n",
    "    def _buildDecoder(self, z, latent_rep_size, max_length, charset_length, gru_layers=3, gru_output_units=501):\n",
    "\n",
    "        h = Dense(latent_rep_size, name='latent_input', activation = 'relu')(z)\n",
    "        h = RepeatVector(max_length, name='repeat_vector')(h)\n",
    "        h = GRU(501, return_sequences = True, name='gru_1')(h)\n",
    "        for gru in range(gru_layers-2):\n",
    "            h = GRU(gru_output_units, return_sequences = True, name='gru_{}'.format(gru+2))(h)\n",
    "        h = GRU(501, return_sequences = True, name='gru_{}'.format(gru_layers))(h)\n",
    "        smiles_decoded = TimeDistributed(Dense(charset_length, activation='softmax'), name='decoded_mean')(h)\n",
    "\n",
    "        return smiles_decoded\n",
    "    \n",
    "    def _buildQSPR(self, z, latent_rep_size, max_length, charset_length):\n",
    "        h = Dense(latent_rep_size, name='latent_input', activation='relu')(z)\n",
    "        h = Dense(100, activation='relu', name='hl_1')(h)\n",
    "        h = Dropout(0.5)(h)\n",
    "        return Dense(1, activation='linear', name='qspr')(h)\n",
    "\n",
    "    def save(self, filename):\n",
    "        self.autoencoder.save_weights(filename)\n",
    "    \n",
    "    def load(self, charset, weights_file, latent_rep_size = 292):\n",
    "        self.create(charset, weights_file = weights_file, latent_rep_size = latent_rep_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcharset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m105\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_rep_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m292\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqspr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgru_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv_filters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv_kernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgru_output_units\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m501\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m <no docstring>\n",
       "\u001b[0;31mFile:\u001b[0m      /media/wesleybeckner/weshhd/wes/Dropbox/Python/py3/generative_learners_salts/notebooks/<ipython-input-2-837442963518>\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vae = MoleculeVAE()\n",
    "vae.create?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.create(char_set, \n",
    "           max_length=51, \n",
    "           latent_rep_size=292,\n",
    "           weights_file='1mil_GDB17_10.h5',\n",
    "           qspr=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "anions = []\n",
    "for smi in salts:\n",
    "    anions.append(smi.split('.')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "anions = []\n",
    "for smi in salts:\n",
    "    anions.append(smi.split('.')[1])\n",
    "lens = []\n",
    "for anion in anions:\n",
    "    lens.append(len(anion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training data\n",
    "#max anion len 62\n",
    "#max cation len 48\n",
    "chemvae = MoleculeVAE()\n",
    "chemvae.create(char_set, max_length=62, conv_layers=3, conv_filters=3, conv_kernel_size=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_47 (InputLayer)           (None, 62, 37)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv_1 (Conv1D)                 (None, 54, 9)        3006        input_47[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_2 (Conv1D)                 (None, 46, 9)        738         conv_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_3 (Conv1D)                 (None, 36, 10)       1000        conv_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 360)          0           conv_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 435)          157035      flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 292)          127312      dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var (Dense)               (None, 292)          127312      dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 292)          0           z_mean[0][0]                     \n",
      "                                                                 z_log_var[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "latent_input (Dense)            (None, 292)          85556       lambda[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector (RepeatVector)    (None, 62, 292)      0           latent_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "gru_1 (GRU)                     (None, 62, 501)      1193382     repeat_vector[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "gru_2 (GRU)                     (None, 62, 501)      1507509     gru_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "gru_3 (GRU)                     (None, 62, 501)      1507509     gru_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "decoded_mean (TimeDistributed)  (None, 62, 37)       18574       gru_3[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 4,728,933\n",
      "Trainable params: 4,728,933\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "chemvae.autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 99000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "99000/99000 [==============================] - 1768s 18ms/step - loss: 2.3151 - acc: 0.7021 - val_loss: 2.2591 - val_acc: 0.7072\n",
      "Train on 99000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "99000/99000 [==============================] - 1768s 18ms/step - loss: 2.2513 - acc: 0.7070 - val_loss: 2.2269 - val_acc: 0.7091\n",
      "Train on 99000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "99000/99000 [==============================] - 1766s 18ms/step - loss: 2.3051 - acc: 0.7029 - val_loss: 2.2560 - val_acc: 0.7078\n",
      "Train on 99000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "99000/99000 [==============================] - 1769s 18ms/step - loss: 2.2421 - acc: 0.7072 - val_loss: 2.2396 - val_acc: 0.7079\n",
      "Train on 99000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "99000/99000 [==============================] - 1790s 18ms/step - loss: 2.2341 - acc: 0.7077 - val_loss: 2.2339 - val_acc: 0.7085\n",
      "Train on 99000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "99000/99000 [==============================] - 1796s 18ms/step - loss: 2.2376 - acc: 0.7074 - val_loss: 2.2198 - val_acc: 0.7082\n",
      "Train on 99000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "99000/99000 [==============================] - 1796s 18ms/step - loss: 2.2353 - acc: 0.7076 - val_loss: 2.2465 - val_acc: 0.7033\n",
      "Train on 99000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "99000/99000 [==============================] - 1813s 18ms/step - loss: 2.2403 - acc: 0.7072 - val_loss: 2.2601 - val_acc: 0.7037\n",
      "Train on 99000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "52416/99000 [==============>...............] - ETA: 14:10 - loss: 2.2539 - acc: 0.7060"
     ]
    }
   ],
   "source": [
    "import json\n",
    "f = open(\"../data/salt_char_to_index.json\",\"r\")\n",
    "char_to_index = json.loads(f.read())\n",
    "char_set = set(char_to_index.keys())\n",
    "char_list = list(char_to_index.keys())\n",
    "chars_in_dict = len(char_list)\n",
    "\n",
    "#training array info\n",
    "smile_max_length = 62\n",
    "df = pd.read_csv('../data/GDB/GDB17.1000000', names=['smiles'])\n",
    "\n",
    "#training data\n",
    "chemvae = MoleculeVAE()\n",
    "chemvae.create(char_set, max_length=62)\n",
    "data_size = 100000\n",
    "histories = []\n",
    "for p in range(10):\n",
    "    values = df['smiles'][data_size*p:data_size*(p+1)]\n",
    "    padded_smiles =  [pad_smiles(i, smile_max_length) for i in values if pad_smiles(i, smile_max_length)]\n",
    "    X_train = np.zeros((data_size, smile_max_length, chars_in_dict), dtype=np.float32)\n",
    "    \n",
    "    #for each i, randomly select whether to sample from GDB or cations (padded_smiles_2)\n",
    "    for i, smile in enumerate(padded_smiles[:data_size]):\n",
    "#         linearly_scaled_prob = random.random() < 0.5#i/data_size\n",
    "#         if linearly_scaled_prob:\n",
    "#             smile = random.choice(cations)\n",
    "        for j, char in enumerate(smile):\n",
    "            X_train[i, j, char_to_index[char]] = 1\n",
    "\n",
    "    X_train, X_test = train_test_split(X_train, test_size=0.01, random_state=42)   \n",
    "    history = chemvae.autoencoder.fit(X_train, X_train, shuffle = False, validation_data=(X_test, X_test))\n",
    "    histories.append(history.history)\n",
    "    chemvae.save('../models/1mil_GDB17_62smi_{}.h5'.format(p+1))\n",
    "    \n",
    "with open('../models/history_{}.json'.format('1mil_GDB17_62smi'), 'w') as f:\n",
    "        json.dump(histories, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 99000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "99000/99000 [==============================] - 2861s 29ms/step - loss: 2.3101 - acc: 0.6382 - val_loss: 2.2386 - val_acc: 0.6445\n",
      "Train on 99000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "99000/99000 [==============================] - 2919s 29ms/step - loss: 2.2505 - acc: 0.6438 - val_loss: 2.2374 - val_acc: 0.6447\n",
      "Train on 99000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "19392/99000 [====>.........................] - ETA: 39:25 - loss: 2.2434 - acc: 0.6443"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-df9b9680c902>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchemvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mhistories\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mchemvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../models/1mil_GDB17_51smi_{}.h5'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wesley/anaconda3/envs/py36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/home/wesley/anaconda3/envs/py36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1234\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1236\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1237\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wesley/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2482\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2483\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wesley/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wesley/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wesley/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wesley/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wesley/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wesley/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "f = open(\"../data/salt_char_to_index.json\",\"r\")\n",
    "char_to_index = json.loads(f.read())\n",
    "char_set = set(char_to_index.keys())\n",
    "char_list = list(char_to_index.keys())\n",
    "chars_in_dict = len(char_list)\n",
    "\n",
    "#training array info\n",
    "smile_max_length = 51\n",
    "df = pd.read_csv('../data/GDB/GDB17.1000000', names=['smiles'])\n",
    "\n",
    "#training data\n",
    "chemvae = MoleculeVAE()\n",
    "chemvae.create(char_set, max_length=51)\n",
    "data_size = 100000\n",
    "histories = []\n",
    "for p in range(10):\n",
    "    values = df['smiles'][data_size*p:data_size*(p+1)]\n",
    "    padded_smiles =  [pad_smiles(i, smile_max_length) for i in values if pad_smiles(i, smile_max_length)]\n",
    "    X_train = np.zeros((data_size, smile_max_length, chars_in_dict), dtype=np.float32)\n",
    "    \n",
    "    #for each i, randomly select whether to sample from GDB or cations (padded_smiles_2)\n",
    "    for i, smile in enumerate(padded_smiles[:data_size]):\n",
    "#         linearly_scaled_prob = random.random() < 0.5#i/data_size\n",
    "#         if linearly_scaled_prob:\n",
    "#             smile = random.choice(cations)\n",
    "        for j, char in enumerate(smile):\n",
    "            X_train[i, j, char_to_index[char]] = 1\n",
    "\n",
    "    X_train, X_test = train_test_split(X_train, test_size=0.01, random_state=42)   \n",
    "    history = chemvae.autoencoder.fit(X_train, X_train, shuffle = False, validation_data=(X_test, X_test))\n",
    "    histories.append(history.history)\n",
    "    chemvae.save('../models/1mil_GDB17_51smi_{}.h5'.format(p+1))\n",
    "    \n",
    "with open('../models/history_{}.json'.format('1mil_GDB17_51smi'), 'w') as f:\n",
    "        json.dump(histories, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 99000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "99000/99000 [==============================] - 2949s 30ms/step - loss: 2.5879 - acc: 0.6383 - val_loss: 2.5128 - val_acc: 0.6441\n",
      "Train on 99000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "99000/99000 [==============================] - 2948s 30ms/step - loss: 2.5196 - acc: 0.6439 - val_loss: 2.4958 - val_acc: 0.6471\n",
      "Train on 99000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "99000/99000 [==============================] - 2999s 30ms/step - loss: 2.5283 - acc: 0.6434 - val_loss: 2.6054 - val_acc: 0.6415\n",
      "Train on 99000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "99000/99000 [==============================] - 3029s 31ms/step - loss: 2.5299 - acc: 0.6430 - val_loss: 2.5025 - val_acc: 0.6460\n",
      "Train on 99000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "99000/99000 [==============================] - 2976s 30ms/step - loss: 2.5269 - acc: 0.6434 - val_loss: 2.5163 - val_acc: 0.6444\n",
      "Train on 99000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "99000/99000 [==============================] - 2969s 30ms/step - loss: 2.5211 - acc: 0.6434 - val_loss: 2.5222 - val_acc: 0.6428\n",
      "Train on 99000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "99000/99000 [==============================] - 2999s 30ms/step - loss: 2.5301 - acc: 0.6430 - val_loss: 2.5838 - val_acc: 0.6378\n",
      "Train on 99000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "99000/99000 [==============================] - 3033s 31ms/step - loss: 2.5161 - acc: 0.6437 - val_loss: 2.5155 - val_acc: 0.6410\n",
      "Train on 99000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "99000/99000 [==============================] - 2986s 30ms/step - loss: 2.5247 - acc: 0.6431 - val_loss: 2.5060 - val_acc: 0.6458\n",
      "Train on 99000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "99000/99000 [==============================] - 3019s 30ms/step - loss: 2.5203 - acc: 0.6436 - val_loss: 2.5077 - val_acc: 0.6441\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "f = open(\"../data/gdb_char_to_index.json\",\"r\")\n",
    "char_to_index = json.loads(f.read())\n",
    "char_set = set(char_to_index.keys())\n",
    "char_list = list(char_to_index.keys())\n",
    "chars_in_dict = len(char_list)\n",
    "\n",
    "#training array info\n",
    "smile_max_length = 51\n",
    "df = pd.read_csv('../data/GDB/GDB17.1000000', names=['smiles'])\n",
    "\n",
    "#training data\n",
    "chemvae = MoleculeVAE()\n",
    "chemvae.create(char_set, max_length=51)\n",
    "data_size = 100000\n",
    "histories = []\n",
    "for p in range(10):\n",
    "    values = df['smiles'][data_size*p:data_size*(p+1)]\n",
    "    padded_smiles =  [pad_smiles(i, smile_max_length) for i in values if pad_smiles(i, smile_max_length)]\n",
    "    X_train = np.zeros((data_size, smile_max_length, chars_in_dict), dtype=np.float32)\n",
    "    \n",
    "    #for each i, randomly select whether to sample from GDB or cations (padded_smiles_2)\n",
    "    for i, smile in enumerate(padded_smiles[:data_size]):\n",
    "#         linearly_scaled_prob = random.random() < 0.5#i/data_size\n",
    "#         if linearly_scaled_prob:\n",
    "#             smile = random.choice(cations)\n",
    "        for j, char in enumerate(smile):\n",
    "            X_train[i, j, char_to_index[char]] = 1\n",
    "\n",
    "    X_train, X_test = train_test_split(X_train, test_size=0.01, random_state=42)   \n",
    "    history = chemvae.autoencoder.fit(X_train, X_train, shuffle = False, validation_data=(X_test, X_test))\n",
    "    histories.append(history.history)\n",
    "    chemvae.save('../models/1mil_GDB17_51smi_gdbchar_{}.h5'.format(p+1))\n",
    "    \n",
    "with open('../models/history_{}.json'.format('1mil_GDB17_51smi_gdbchar'), 'w') as f:\n",
    "        json.dump(histories, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#supporting functions\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "from scripts import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 99000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "99000/99000 [==============================] - 5646s 57ms/step - loss: 2.3607 - acc: 0.6542 - val_loss: 2.1776 - val_acc: 0.6833\n",
      "Train on 99000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "99000/99000 [==============================] - 5677s 57ms/step - loss: 1.6147 - acc: 0.7606 - val_loss: 1.3074 - val_acc: 0.8074\n",
      "Train on 99000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "99000/99000 [==============================] - 5702s 58ms/step - loss: 1.1788 - acc: 0.8273 - val_loss: 1.0910 - val_acc: 0.8419\n",
      "Train on 99000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "99000/99000 [==============================] - 5702s 58ms/step - loss: 0.9733 - acc: 0.8591 - val_loss: 0.9107 - val_acc: 0.8683\n",
      "Train on 99000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "99000/99000 [==============================] - 5758s 58ms/step - loss: 0.8342 - acc: 0.8805 - val_loss: 0.8692 - val_acc: 0.8787\n",
      "Train on 99000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "99000/99000 [==============================] - 5774s 58ms/step - loss: 0.7345 - acc: 0.8960 - val_loss: 0.6597 - val_acc: 0.9078\n",
      "Train on 99000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "99000/99000 [==============================] - 4797s 48ms/step - loss: 0.6625 - acc: 0.9075 - val_loss: 0.5991 - val_acc: 0.9154\n",
      "Train on 99000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "99000/99000 [==============================] - 2994s 30ms/step - loss: 0.6000 - acc: 0.9168 - val_loss: 0.5752 - val_acc: 0.9203\n",
      "Train on 99000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "99000/99000 [==============================] - 2974s 30ms/step - loss: 0.5471 - acc: 0.9247 - val_loss: 0.5379 - val_acc: 0.9260\n",
      "Train on 99000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "99000/99000 [==============================] - 2986s 30ms/step - loss: 0.5114 - acc: 0.9300 - val_loss: 0.4570 - val_acc: 0.9375\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "f = open(\"../data/gdb_char_to_index.json\",\"r\")\n",
    "char_to_index = json.loads(f.read())\n",
    "char_set = set(char_to_index.keys())\n",
    "char_list = list(char_to_index.keys())\n",
    "chars_in_dict = len(char_list)\n",
    "\n",
    "#training array info\n",
    "smile_max_length = 51\n",
    "df = pd.read_csv('../data/GDB/GDB17.1000000', names=['smiles'])\n",
    "\n",
    "#training data\n",
    "chemvae = MoleculeVAE()\n",
    "chemvae.create(char_set, max_length=51)\n",
    "data_size = 100000\n",
    "histories = []\n",
    "for p in range(10):\n",
    "    values = df['smiles'][data_size*p:data_size*(p+1)]\n",
    "    padded_smiles =  [pad_smiles(i, smile_max_length) for i in values if pad_smiles(i, smile_max_length)]\n",
    "    X_train = np.zeros((data_size, smile_max_length, chars_in_dict), dtype=np.float32)\n",
    "    \n",
    "    #for each i, randomly select whether to sample from GDB or cations (padded_smiles_2)\n",
    "    for i, smile in enumerate(padded_smiles[:data_size]):\n",
    "#         linearly_scaled_prob = random.random() < 0.5#i/data_size\n",
    "#         if linearly_scaled_prob:\n",
    "#             smile = random.choice(cations)\n",
    "        for j, char in enumerate(smile):\n",
    "            X_train[i, j, char_to_index[char]] = 1\n",
    "\n",
    "    X_train, X_test = train_test_split(X_train, test_size=0.01, random_state=42)   \n",
    "    history = chemvae.autoencoder.fit(X_train, X_train, shuffle = False, validation_data=(X_test, X_test))\n",
    "    histories.append(history.history)\n",
    "#     chemvae.save('../models/1mil_GDB17_51smi_gdbchar_{}.h5'.format(p+1))\n",
    "    \n",
    "# with open('../models/history_{}.json'.format('1mil_GDB17_51smi_gdbchar'), 'w') as f:\n",
    "#         json.dump(histories, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 99000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "99000/99000 [==============================] - 2848s 29ms/step - loss: 2.1012 - acc: 0.6518 - val_loss: 1.8331 - val_acc: 0.6810\n",
      "Train on 99000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "99000/99000 [==============================] - 2861s 29ms/step - loss: 1.4858 - acc: 0.7541 - val_loss: 1.1758 - val_acc: 0.8094\n",
      "Train on 99000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "99000/99000 [==============================] - 2858s 29ms/step - loss: 1.0317 - acc: 0.8357 - val_loss: 0.9028 - val_acc: 0.8567\n",
      "Train on 99000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "99000/99000 [==============================] - 2845s 29ms/step - loss: 0.8354 - acc: 0.8680 - val_loss: 0.8972 - val_acc: 0.8608\n",
      "Train on 99000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "99000/99000 [==============================] - 2871s 29ms/step - loss: 0.7093 - acc: 0.8885 - val_loss: 0.6087 - val_acc: 0.9056\n",
      "Train on 99000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "99000/99000 [==============================] - 2866s 29ms/step - loss: 0.6279 - acc: 0.9024 - val_loss: 0.6379 - val_acc: 0.9019\n",
      "Train on 99000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "99000/99000 [==============================] - 2915s 29ms/step - loss: 0.5766 - acc: 0.9111 - val_loss: 0.5767 - val_acc: 0.9110\n",
      "Train on 99000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "99000/99000 [==============================] - 2911s 29ms/step - loss: 0.5111 - acc: 0.9218 - val_loss: 0.4724 - val_acc: 0.9278\n",
      "Train on 99000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "99000/99000 [==============================] - 2970s 30ms/step - loss: 0.4773 - acc: 0.9278 - val_loss: 0.4664 - val_acc: 0.9298\n",
      "Train on 99000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "99000/99000 [==============================] - 2962s 30ms/step - loss: 0.4425 - acc: 0.9336 - val_loss: 0.3870 - val_acc: 0.9412\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "f = open(\"../data/salt_char_to_index.json\",\"r\")\n",
    "char_to_index = json.loads(f.read())\n",
    "char_set = set(char_to_index.keys())\n",
    "char_list = list(char_to_index.keys())\n",
    "chars_in_dict = len(char_list)\n",
    "\n",
    "#training array info\n",
    "smile_max_length = 51\n",
    "df = pd.read_csv('../data/GDB/GDB17.1000000', names=['smiles'])\n",
    "\n",
    "#training data\n",
    "chemvae = MoleculeVAE()\n",
    "chemvae.create(char_set, max_length=51)\n",
    "data_size = 100000\n",
    "histories = []\n",
    "for p in range(10):\n",
    "    values = df['smiles'][data_size*p:data_size*(p+1)]\n",
    "    padded_smiles =  [pad_smiles(i, smile_max_length) for i in values if pad_smiles(i, smile_max_length)]\n",
    "    X_train = np.zeros((data_size, smile_max_length, chars_in_dict), dtype=np.float32)\n",
    "    \n",
    "    #for each i, randomly select whether to sample from GDB or cations (padded_smiles_2)\n",
    "    for i, smile in enumerate(padded_smiles[:data_size]):\n",
    "#         linearly_scaled_prob = random.random() < 0.5#i/data_size\n",
    "#         if linearly_scaled_prob:\n",
    "#             smile = random.choice(cations)\n",
    "        for j, char in enumerate(smile):\n",
    "            X_train[i, j, char_to_index[char]] = 1\n",
    "\n",
    "    X_train, X_test = train_test_split(X_train, test_size=0.01, random_state=42)   \n",
    "    history = chemvae.autoencoder.fit(X_train, X_train, shuffle = False, validation_data=(X_test, X_test))\n",
    "    histories.append(history.history)\n",
    "    chemvae.save('../models/1mil_GDB17_51smi_saltchar_{}.h5'.format(p+1))\n",
    "    \n",
    "with open('../models/history_{}.json'.format('1mil_GDB17_51smi_saltchar'), 'w') as f:\n",
    "        json.dump(histories, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 99000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "99000/99000 [==============================] - 3513s 35ms/step - loss: 2.1830 - acc: 0.7084 - val_loss: 2.0775 - val_acc: 0.7137\n",
      "Train on 99000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "99000/99000 [==============================] - 3485s 35ms/step - loss: 1.7511 - acc: 0.7590 - val_loss: 1.5070 - val_acc: 0.7927\n",
      "Train on 99000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "99000/99000 [==============================] - 3482s 35ms/step - loss: 1.4245 - acc: 0.8046 - val_loss: 1.3079 - val_acc: 0.8220\n",
      "Train on 99000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "99000/99000 [==============================] - 3499s 35ms/step - loss: 1.2317 - acc: 0.8315 - val_loss: 1.1965 - val_acc: 0.8361\n",
      "Train on 99000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "99000/99000 [==============================] - 3537s 36ms/step - loss: 1.5026 - acc: 0.7982 - val_loss: 1.6258 - val_acc: 0.7822\n",
      "Train on 99000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "99000/99000 [==============================] - 3526s 36ms/step - loss: 1.3948 - acc: 0.8136 - val_loss: 1.2069 - val_acc: 0.8395\n",
      "Train on 99000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "99000/99000 [==============================] - 3528s 36ms/step - loss: 1.1492 - acc: 0.8470 - val_loss: 1.0988 - val_acc: 0.8533\n",
      "Train on 99000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "99000/99000 [==============================] - 3598s 36ms/step - loss: 1.2024 - acc: 0.8402 - val_loss: 1.0743 - val_acc: 0.8560\n",
      "Train on 99000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "99000/99000 [==============================] - 3588s 36ms/step - loss: 1.0033 - acc: 0.8658 - val_loss: 0.9538 - val_acc: 0.8719\n",
      "Train on 99000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "99000/99000 [==============================] - 3579s 36ms/step - loss: 0.9374 - acc: 0.8751 - val_loss: 0.9079 - val_acc: 0.8780\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "f = open(\"../data/salt_char_to_index.json\",\"r\")\n",
    "char_to_index = json.loads(f.read())\n",
    "char_set = set(char_to_index.keys())\n",
    "char_list = list(char_to_index.keys())\n",
    "chars_in_dict = len(char_list)\n",
    "\n",
    "#training array info\n",
    "smile_max_length = 62\n",
    "df = pd.read_csv('../data/GDB/GDB17.1000000', names=['smiles'])\n",
    "\n",
    "#training data\n",
    "chemvae = MoleculeVAE()\n",
    "chemvae.create(char_set, max_length=62)\n",
    "data_size = 100000\n",
    "histories = []\n",
    "for p in range(10):\n",
    "    values = df['smiles'][data_size*p:data_size*(p+1)]\n",
    "    padded_smiles =  [pad_smiles(i, smile_max_length) for i in values if pad_smiles(i, smile_max_length)]\n",
    "    X_train = np.zeros((data_size, smile_max_length, chars_in_dict), dtype=np.float32)\n",
    "    \n",
    "    #for each i, randomly select whether to sample from GDB or cations (padded_smiles_2)\n",
    "    for i, smile in enumerate(padded_smiles[:data_size]):\n",
    "#         linearly_scaled_prob = random.random() < 0.5#i/data_size\n",
    "#         if linearly_scaled_prob:\n",
    "#             smile = random.choice(cations)\n",
    "        for j, char in enumerate(smile):\n",
    "            X_train[i, j, char_to_index[char]] = 1\n",
    "\n",
    "    X_train, X_test = train_test_split(X_train, test_size=0.01, random_state=42)   \n",
    "    history = chemvae.autoencoder.fit(X_train, X_train, shuffle = False, validation_data=(X_test, X_test))\n",
    "    histories.append(history.history)\n",
    "    chemvae.save('../models/1mil_GDB17_62smi_saltchar_{}.h5'.format(p+1))\n",
    "    \n",
    "with open('../models/history_{}.json'.format('1mil_GDB17_62smi_saltchar'), 'w') as f:\n",
    "        json.dump(histories, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "anions = []\n",
    "for anion in salts:\n",
    "    anions.append(anion.split('.')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "anions = pd.DataFrame(anions)\n",
    "anions.columns = ['smiles']\n",
    "anions = anions.drop_duplicates()\n",
    "anions = anions.reset_index(drop=True)\n",
    "anions = anions['smiles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "chemvae = MoleculeVAE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mchemvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcharset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m105\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_rep_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m292\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqspr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m <no docstring>\n",
       "\u001b[0;31mFile:\u001b[0m      ~/Dropbox/Python/py3/generative_learners_salts/scripts/vae.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chemvae.create?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anions added: 50098\n",
      "Train on 100000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 3681s 37ms/step - loss: 0.5418 - acc: 0.9314 - val_loss: 0.5815 - val_acc: 0.9245\n",
      "anions added: 50081\n",
      "Train on 100000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 3617s 36ms/step - loss: 0.4996 - acc: 0.9353 - val_loss: 0.7252 - val_acc: 0.9139\n",
      "anions added: 49991\n",
      "Train on 100000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 3614s 36ms/step - loss: 0.4834 - acc: 0.9375 - val_loss: 0.5096 - val_acc: 0.9335\n",
      "anions added: 49955\n",
      "Train on 100000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 3606s 36ms/step - loss: 0.4477 - acc: 0.9420 - val_loss: 0.5087 - val_acc: 0.9343\n",
      "anions added: 50138\n",
      "Train on 100000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 3615s 36ms/step - loss: 0.4505 - acc: 0.9423 - val_loss: 1.0113 - val_acc: 0.8815\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "f = open(\"../data/salt_char_to_index.json\",\"r\")\n",
    "char_to_index = json.loads(f.read())\n",
    "char_set = set(char_to_index.keys())\n",
    "char_list = list(char_to_index.keys())\n",
    "chars_in_dict = len(char_list)\n",
    "\n",
    "#training array info\n",
    "smile_max_length = 62\n",
    "df = pd.read_csv('../data/GDB/GDB17.2000000', names=['smiles'])\n",
    "\n",
    "#training data\n",
    "chemvae = MoleculeVAE()\n",
    "chemvae.create(char_set, max_length=62, weights_file='../models/1mil_GDB17_62smi_saltchar_10.h5')\n",
    "data_size = 100000\n",
    "histories = []\n",
    "for p in range(5):\n",
    "    values = df['smiles'][data_size*p:data_size*(p+1)]\n",
    "    padded_smiles =  [pad_smiles(i, smile_max_length) for i in values if pad_smiles(i, smile_max_length)]\n",
    "    padded_anions =  [pad_smiles(i, smile_max_length) for i in anions if pad_smiles(i, smile_max_length)]\n",
    "    X_train = np.zeros((data_size, smile_max_length, chars_in_dict), dtype=np.float32)\n",
    "    \n",
    "    #for each i, randomly select whether to sample from GDB or cations (padded_smiles_2)\n",
    "    anions_added = 0\n",
    "    for i, smile in enumerate(padded_smiles):\n",
    "        linearly_scaled_prob = random.random() < i/data_size\n",
    "        if linearly_scaled_prob:\n",
    "            smile = random.choice(padded_anions)\n",
    "            anions_added += 1\n",
    "        for j, char in enumerate(smile):\n",
    "            X_train[i, j, char_to_index[char]] = 1\n",
    "    print('anions added: {}'.format(anions_added))\n",
    "\n",
    "#     X_train, X_test = train_test_split(X_train, test_size=0.01, random_state=42)   \n",
    "    history = chemvae.autoencoder.fit(X_train, X_train, shuffle = False, validation_data=(X_test, X_test))\n",
    "    histories.append(history.history)\n",
    "    chemvae.save('../models/1mil_GDB17_62smi_saltchar_500k_mix_{}.h5'.format(p+1))\n",
    "    \n",
    "with open('../models/history_{}.json'.format('1mil_GDB17_62smi_saltchar_500k_mix'), 'w') as f:\n",
    "        json.dump(histories, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anions added: 100000\n",
      "Train on 100000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 3601s 36ms/step - loss: 0.0491 - acc: 0.9958 - val_loss: 3.9338 - val_acc: 0.8143\n",
      "anions added: 100000\n",
      "Train on 100000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 3581s 36ms/step - loss: 0.0395 - acc: 0.9965 - val_loss: 4.9384 - val_acc: 0.8041\n",
      "anions added: 100000\n",
      "Train on 100000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 3548s 35ms/step - loss: 0.0835 - acc: 0.9918 - val_loss: 3.9590 - val_acc: 0.8081\n",
      "anions added: 100000\n",
      "Train on 100000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 3552s 36ms/step - loss: 0.0826 - acc: 0.9918 - val_loss: 4.9319 - val_acc: 0.7929\n",
      "anions added: 100000\n",
      "Train on 100000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 3527s 35ms/step - loss: 0.3351 - acc: 0.9644 - val_loss: 5.7355 - val_acc: 0.7608\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "f = open(\"../data/salt_char_to_index.json\",\"r\")\n",
    "char_to_index = json.loads(f.read())\n",
    "char_set = set(char_to_index.keys())\n",
    "char_list = list(char_to_index.keys())\n",
    "chars_in_dict = len(char_list)\n",
    "\n",
    "#training array info\n",
    "smile_max_length = 62\n",
    "df = pd.read_csv('../data/GDB/GDB17.2000000', names=['smiles'])\n",
    "\n",
    "#training data\n",
    "chemvae = MoleculeVAE()\n",
    "chemvae.create(char_set, max_length=62, weights_file='../models/1mil_GDB17_62smi_saltchar_500k_mix_5.h5')\n",
    "data_size = 100000\n",
    "histories = []\n",
    "for p in range(5):\n",
    "    values = df['smiles'][data_size*p:data_size*(p+1)]\n",
    "    padded_smiles =  [pad_smiles(i, smile_max_length) for i in values if pad_smiles(i, smile_max_length)]\n",
    "    padded_anions =  [pad_smiles(i, smile_max_length) for i in anions if pad_smiles(i, smile_max_length)]\n",
    "    X_train = np.zeros((data_size, smile_max_length, chars_in_dict), dtype=np.float32)\n",
    "    \n",
    "    #for each i, randomly select whether to sample from GDB or cations (padded_smiles_2)\n",
    "    anions_added = 0\n",
    "    for i, smile in enumerate(padded_smiles):\n",
    "#         linearly_scaled_prob = random.random() < i/data_size\n",
    "#         if linearly_scaled_prob:\n",
    "        smile = random.choice(padded_anions)\n",
    "        anions_added += 1\n",
    "        for j, char in enumerate(smile):\n",
    "            X_train[i, j, char_to_index[char]] = 1\n",
    "    print('anions added: {}'.format(anions_added))\n",
    "\n",
    "#     X_train, X_test = train_test_split(X_train, test_size=0.01, random_state=42)   \n",
    "    history = chemvae.autoencoder.fit(X_train, X_train, shuffle = False, validation_data=(X_test, X_test))\n",
    "    histories.append(history.history)\n",
    "    chemvae.save('../models/1mil_GDB17_62smi_saltchar_500k_mix_anion_{}.h5'.format(p+1))\n",
    "    \n",
    "with open('../models/history_{}.json'.format('1mil_GDB17_62smi_saltchar_500k_mix_anion'), 'w') as f:\n",
    "        json.dump(histories, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "cationvae = MoleculeVAE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mcationvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcharset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m105\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_rep_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m292\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqspr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m <no docstring>\n",
       "\u001b[0;31mFile:\u001b[0m      ~/Dropbox/Python/py3/generative_learners_salts/scripts/vae.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cationvae.create?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training array info\n",
    "import json\n",
    "f = open(\"../data/gdb_char_to_index.json\",\"r\")\n",
    "cat_char_to_index = json.loads(f.read())\n",
    "cat_char_set = set(cat_char_to_index.keys())\n",
    "cat_char_list = list(cat_char_to_index.keys())\n",
    "cat_chars_in_dict = len(cat_char_list)\n",
    "cationvae.create(cat_char_set, max_length=51, weights_file='../models/1mil_GDB17_500K_mix_500K_cation_5.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "anionvae = chemvae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.topology.InputLayer at 0x7fdfccb2eba8>,\n",
       " <keras.layers.convolutional.Conv1D at 0x7fdfcca6bcf8>,\n",
       " <keras.layers.convolutional.Conv1D at 0x7fdfcca6beb8>,\n",
       " <keras.layers.convolutional.Conv1D at 0x7fdfcca22470>,\n",
       " <keras.layers.core.Flatten at 0x7fdfcc9d4da0>,\n",
       " <keras.layers.core.Dense at 0x7fdfcc9f9400>,\n",
       " <keras.layers.core.Dense at 0x7fdfcc9f9d30>,\n",
       " <keras.layers.core.Dense at 0x7fdfcc9bdac8>,\n",
       " <keras.layers.core.Lambda at 0x7fdfcc954eb8>,\n",
       " <keras.layers.core.Dense at 0x7fdfcc969358>,\n",
       " <keras.layers.core.RepeatVector at 0x7fdfcc903358>,\n",
       " <keras.layers.recurrent.GRU at 0x7fdfcc916898>,\n",
       " <keras.layers.recurrent.GRU at 0x7fdfcc8c5a20>,\n",
       " <keras.layers.recurrent.GRU at 0x7fdfcc811518>,\n",
       " <keras.layers.wrappers.TimeDistributed at 0x7fdfcc506c18>]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anionvae.autoencoder.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
