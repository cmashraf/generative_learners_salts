{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import seaborn as sns; sns.set()\n",
    "%matplotlib inline\n",
    "\n",
    "import keras\n",
    "from keras import objectives\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Input, Multiply, Add\n",
    "from keras.optimizers import Adam, Nadam\n",
    "import salty\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from random import shuffle\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "#Keras build\n",
    "from keras import backend as K\n",
    "from keras.objectives import binary_crossentropy #objs or losses\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Lambda, Layer\n",
    "from keras.layers.core import Dense, Activation, Flatten, RepeatVector\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.layers.convolutional import Convolution1D\n",
    "\n",
    "#cation data\n",
    "cations = pd.read_csv('../data/cations.csv')\n",
    "cations = cations['smiles_string']\n",
    "salts = pd.read_csv('../data/salts.csv')\n",
    "salts = salts['smiles_string']\n",
    "categories = pd.read_csv('../data/categories.csv')\n",
    "categories = categories['category']\n",
    "coldic = pd.read_csv('../data/coldic.csv')\n",
    "coldic = coldic.to_dict(orient='records')[0]\n",
    "\n",
    "#supporting functions\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "from scripts import build_vae, decode_smiles, generate_structures, my_colors, MoleculeVAE, one_hot\n",
    "\n",
    "def pad_smiles(smiles_string, smile_max_length):\n",
    "     if len(smiles_string) < smile_max_length:\n",
    "            return smiles_string + \" \" * (smile_max_length - len(smiles_string))\n",
    "        \n",
    "def create_char_list(char_set, smile_series):\n",
    "    for smile in smile_series:\n",
    "        char_set.update(set(smile))\n",
    "    return char_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CCC(N)C(C)C1(CC#C)CC(CC)N=CN1.CC1=C2N=CC(NCC=O)=C(F)N2C(C=O)=N1\n",
      "CC1=C2N=CC(NCC=O)=C(F)N2C(C=O)=N1.CCC(N)C(C)C1(CC#C)CC(CC)N=CN1\n",
      "100000\n",
      "Train on 90000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "90000/90000 [==============================] - 2757s 31ms/step - loss: 4.7901 - acc: 0.6336 - val_loss: 4.3106 - val_acc: 0.6578\n",
      "CC(CN)C1=CN2C(N1)=C(Br)C(N)=C(Br)C2=N.CC12OC3C1C(C=C2)N1CCC2=CSC3=C12\n",
      "CC12OC3C1C(C=C2)N1CCC2=CSC3=C12.CC(CN)C1=CN2C(N1)=C(Br)C(N)=C(Br)C2=N\n",
      "100000\n",
      "Train on 90000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "90000/90000 [==============================] - 2772s 31ms/step - loss: 4.2467 - acc: 0.6585 - val_loss: 4.1893 - val_acc: 0.6611\n",
      "CC12COC(=O)N1C1(CS(=O)(=O)NC21C)C=O.CC1C=CCC1NCCCNCC(C)=NO\n",
      "CC1C=CCC1NCCCNCC(C)=NO.CC12COC(=O)N1C1(CS(=O)(=O)NC21C)C=O\n",
      "100000\n",
      "Train on 90000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "90000/90000 [==============================] - 2768s 31ms/step - loss: 4.1297 - acc: 0.6635 - val_loss: 4.1782 - val_acc: 0.6604\n",
      "CC1(C=C2C(O)C3NCC1(C)C23N)C#C.CCC#CC1C=CN(C1=O)C1=CCOC1=N\n",
      "CCC#CC1C=CN(C1=O)C1=CCOC1=N.CC1(C=C2C(O)C3NCC1(C)C23N)C#C\n",
      "100000\n",
      "Train on 90000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "90000/90000 [==============================] - 2762s 31ms/step - loss: 4.0646 - acc: 0.6670 - val_loss: 4.0601 - val_acc: 0.6683\n",
      "CCC(C)(O)C1=CC2COC1(C=O)S2(=O)=O.CC1=CC2=C(N=CC3=C2NC(N)=NN3)C1=NO\n",
      "CC1=CC2=C(N=CC3=C2NC(N)=NN3)C1=NO.CCC(C)(O)C1=CC2COC1(C=O)S2(=O)=O\n",
      "100000\n",
      "Train on 90000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "90000/90000 [==============================] - 2778s 31ms/step - loss: 3.9982 - acc: 0.6718 - val_loss: 4.0053 - val_acc: 0.6703\n",
      "CCC1(C)NC(COC(OC)C1C#N)C(N)=N.CC1(C)COC(C2=C3SC=NC3=NS2)C(N)=N1\n",
      "CC1(C)COC(C2=C3SC=NC3=NS2)C(N)=N1.CCC1(C)NC(COC(OC)C1C#N)C(N)=N\n",
      "100000\n",
      "Train on 90000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "90000/90000 [==============================] - 2773s 31ms/step - loss: 3.9358 - acc: 0.6767 - val_loss: 3.8705 - val_acc: 0.6829\n",
      "CC1(OC(CN)C#C)OC(=N)NCC1(N)CO.CC12CC(CO1)C(=O)CCC2(C)C(O)CC#C\n",
      "CC12CC(CO1)C(=O)CCC2(C)C(O)CC#C.CC1(OC(CN)C#C)OC(=N)NCC1(N)CO\n",
      "100000\n",
      "Train on 90000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "90000/90000 [==============================] - 2796s 31ms/step - loss: 4.2914 - acc: 0.6618 - val_loss: 4.6948 - val_acc: 0.6437\n",
      "CC1CC(N)CNCC11OCOC(=O)NC1=O.BrC1=C2C(=N)SC=C2C=C(N1)C(=O)C1CN1\n",
      "BrC1=C2C(=N)SC=C2C=C(N1)C(=O)C1CN1.CC1CC(N)CNCC11OCOC(=O)NC1=O\n",
      "100000\n",
      "Train on 90000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "90000/90000 [==============================] - 2785s 31ms/step - loss: 4.6455 - acc: 0.6443 - val_loss: 4.6440 - val_acc: 0.6448\n",
      "CCC(=NO)C1=C(I)C2=COC=C2N=C1OC.CC1CC2(CNC1C1CC2N=C(O1)C#C)C#N\n",
      "CC1CC2(CNC1C1CC2N=C(O1)C#C)C#N.CCC(=NO)C1=C(I)C2=COC=C2N=C1OC\n",
      "100000\n",
      "Train on 90000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "90000/90000 [==============================] - 2784s 31ms/step - loss: 4.5828 - acc: 0.6454 - val_loss: 4.5633 - val_acc: 0.6459\n",
      "CC12COC(C3NC3C3=CC=NC=C13)S2(=O)=O.CC12CC(N1)C=C2C1=C(Br)C=CSC1=O\n",
      "CC12CC(N1)C=C2C1=C(Br)C=CSC1=O.CC12COC(C3NC3C3=CC=NC=C13)S2(=O)=O\n",
      "99999\n",
      "Train on 90000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "90000/90000 [==============================] - 2764s 31ms/step - loss: 4.5406 - acc: 0.6460 - val_loss: 4.4609 - val_acc: 0.6493\n"
     ]
    }
   ],
   "source": [
    "#ROUND 1: GDB TRAINING BY THE 100K\n",
    "\n",
    "#training array info\n",
    "smile_max_length = 105\n",
    "import json\n",
    "f = open(\"../data/1mil_GDB17.json\",\"r\")\n",
    "char_to_index = json.loads(f.read())\n",
    "\n",
    "#chars needed for 2 mol w/ anions\n",
    "char_to_index['.'] = 33\n",
    "char_to_index['@'] = 34\n",
    "char_to_index['e'] = 35\n",
    "char_to_index['/'] = 36\n",
    "\n",
    "char_set = set(char_to_index.keys())\n",
    "char_list = list(char_to_index.keys())\n",
    "chars_in_dict = len(char_list)\n",
    "\n",
    "#training data\n",
    "df = pd.read_csv('../data/GDB/GDB17.1000000', names=['smiles'])\n",
    "data_size = 100000\n",
    "max_data = df.shape[0]\n",
    "chemvae = MoleculeVAE()\n",
    "chemvae.create(char_set, max_length=smile_max_length)\n",
    "\n",
    "for p in range(0,10):\n",
    "    selected = df['smiles'][data_size*p:data_size*(p+1)].reset_index(drop=True)\n",
    "    values = pd.Series(selected.values + '.' + selected[::-1].values)\n",
    "    print(values[0])\n",
    "    print(values.iloc[-1])\n",
    "    print(len(values))\n",
    "    padded_smiles =  [pad_smiles(i, smile_max_length) for i in values if pad_smiles(i, smile_max_length)]\n",
    "    X_train = np.zeros((data_size, smile_max_length, chars_in_dict), dtype=np.float32)\n",
    "    \n",
    "    for i, smile in enumerate(padded_smiles):\n",
    "        for j, char in enumerate(smile):\n",
    "            X_train[i, j, char_to_index[char]] = 1\n",
    "    X_train, X_test = train_test_split(X_train, test_size=0.1, random_state=42) \n",
    "    chemvae.autoencoder.fit(X_train, X_train, shuffle = False, validation_data=(X_test, X_test))\n",
    "    chemvae.save('gen2_2mol_1mil_GDB17_{}.h5'.format(p+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROUND 2: GDB TRAINING BY THE 500K\n",
    "\n",
    "#training array info\n",
    "smile_max_length = 105\n",
    "import json\n",
    "f = open(\"../data/1mil_GDB17.json\",\"r\")\n",
    "char_to_index = json.loads(f.read())\n",
    "\n",
    "#chars needed for 2 mol w/ anions\n",
    "char_to_index['.'] = 33\n",
    "char_to_index['@'] = 34\n",
    "char_to_index['e'] = 35\n",
    "char_to_index['/'] = 36\n",
    "\n",
    "char_set = set(char_to_index.keys())\n",
    "char_list = list(char_to_index.keys())\n",
    "chars_in_dict = len(char_list)\n",
    "index_to_char = dict((i, c) for i, c in enumerate(char_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoleculeVAE():\n",
    "\n",
    "    autoencoder = None\n",
    "    \n",
    "    def create(self,\n",
    "               charset,\n",
    "               max_length = 105,\n",
    "               latent_rep_size = 292,\n",
    "               weights_file = None):\n",
    "        charset_length = len(charset)\n",
    "        \n",
    "        x = Input(shape=(max_length, charset_length))\n",
    "        _, z = self._buildEncoder(x, latent_rep_size, max_length)\n",
    "        self.encoder = Model(x, z)\n",
    "\n",
    "        encoded_input = Input(shape=(latent_rep_size,))\n",
    "        self.decoder = Model(\n",
    "            encoded_input,\n",
    "            self._buildDecoder(\n",
    "                encoded_input,\n",
    "                latent_rep_size,\n",
    "                max_length,\n",
    "                charset_length\n",
    "            )\n",
    "        )\n",
    "\n",
    "        x1 = Input(shape=(max_length, charset_length))\n",
    "        vae_loss, z1 = self._buildEncoder(x1, latent_rep_size, max_length)\n",
    "        self.autoencoder = Model(\n",
    "            x1,\n",
    "            self._buildDecoder(\n",
    "                z1,\n",
    "                latent_rep_size,\n",
    "                max_length,\n",
    "                charset_length\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.qspr = Model(\n",
    "            x1,\n",
    "            self._buildQSPR(\n",
    "                z1,\n",
    "                latent_rep_size,\n",
    "                max_length,\n",
    "                charset_length\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if weights_file:\n",
    "            self.autoencoder.load_weights(weights_file, by_name = True)\n",
    "            self.encoder.load_weights(weights_file, by_name = True)\n",
    "            self.decoder.load_weights(weights_file, by_name = True)\n",
    "            self.qspr.load_weights(weights_file, by_name = True)\n",
    "\n",
    "        self.autoencoder.compile(optimizer = 'Adam',\n",
    "                                 loss = {'decoded_mean': vae_loss, 'qspr': 'mean_squared_error'},\n",
    "                                 metrics = ['accuracy', 'mse'])\n",
    "\n",
    "    def _buildEncoder(self, x, latent_rep_size, max_length, epsilon_std = 0.01):\n",
    "        h = Convolution1D(9, 9, activation = 'relu', name='conv_1')(x)\n",
    "        h = Convolution1D(9, 9, activation = 'relu', name='conv_2')(h)\n",
    "        h = Convolution1D(10, 11, activation = 'relu', name='conv_3')(h)\n",
    "        h = Flatten(name='flatten_1')(h)\n",
    "        h = Dense(435, activation = 'relu', name='dense_1')(h)\n",
    "\n",
    "        def sampling(args):\n",
    "            z_mean_, z_log_var_ = args\n",
    "            batch_size = K.shape(z_mean_)[0]\n",
    "            epsilon = K.random_normal(shape=(batch_size, latent_rep_size), mean=0., stddev = epsilon_std)\n",
    "            return z_mean_ + K.exp(z_log_var_ / 2) * epsilon\n",
    "\n",
    "        z_mean = Dense(latent_rep_size, name='z_mean', activation = 'linear')(h)\n",
    "        z_log_var = Dense(latent_rep_size, name='z_log_var', activation = 'linear')(h)\n",
    "\n",
    "        def vae_loss(x, x_decoded_mean):\n",
    "            x = K.flatten(x)\n",
    "            x_decoded_mean = K.flatten(x_decoded_mean)\n",
    "            xent_loss = max_length * objectives.binary_crossentropy(x, x_decoded_mean)\n",
    "            kl_loss = - 0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis = -1)\n",
    "            return xent_loss + kl_loss\n",
    "\n",
    "        return (vae_loss, Lambda(sampling, output_shape=(latent_rep_size,), name='lambda')([z_mean, z_log_var]))\n",
    "\n",
    "    def _buildDecoder(self, z, latent_rep_size, max_length, charset_length):\n",
    "        \n",
    "        h = Dense(latent_rep_size, name='latent_input', activation = 'relu')(z)\n",
    "        h = RepeatVector(max_length, name='repeat_vector')(h)\n",
    "        h = GRU(501, return_sequences = True, name='gru_1')(h)\n",
    "        h = GRU(501, return_sequences = True, name='gru_2')(h)\n",
    "        h = GRU(501, return_sequences = True, name='gru_3')(h)\n",
    "        smiles_decoded = TimeDistributed(Dense(charset_length, activation='softmax'), name='decoded_mean')(h)\n",
    "        \n",
    "        h = Dense(latent_rep_size, name='qspr_input', activation='relu')(z)\n",
    "        h = Dense(100, activation='relu', name='hl_1')(h)\n",
    "        h = Dropout(0.5)(h)\n",
    "        smiles_qspr = Dense(1, activation='linear', name='qspr')(h)\n",
    "        \n",
    "        return smiles_decoded, smiles_qspr\n",
    "    \n",
    "    def _buildQSPR(self, z, latent_rep_size, max_length, charset_length):\n",
    "        h = Dense(latent_rep_size, name='latent_input', activation='relu')(z)\n",
    "        h = Dense(100, activation='relu', name='hl_1')(h)\n",
    "        h = Dropout(0.5)(h)\n",
    "        return Dense(1, activation='linear', name='qspr')(h)\n",
    "\n",
    "    def save(self, filename):\n",
    "        self.autoencoder.save_weights(filename)\n",
    "    \n",
    "    def load(self, charset, weights_file, latent_rep_size = 292):\n",
    "        self.create(charset, weights_file = weights_file, latent_rep_size = latent_rep_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize weights\n",
    "chemvae = MoleculeVAE()\n",
    "chemvae.create(char_set, max_length=smile_max_length, weights_file='gen2_2mol_50mil_GDB17_5_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/GDB/GDB17.1000000')\n",
    "df.columns = ['smiles']\n",
    "data_epoch_size = 500000 #500K\n",
    "for _epoch in range(1):\n",
    "    selected = df['smiles'][data_epoch_size*_epoch:data_epoch_size*(_epoch+1)].reset_index(drop=True)\n",
    "    values = pd.Series(selected.values + '.' + selected[::-1].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts import sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CC(CCC(CCCCCC(CCC(CC)CCCCCC.1CC1CCCCCCN)1CC=CC1CCCC(O)=O                                                 '"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = \"\"\n",
    "for i in chemvae.autoencoder.predict(one_hot(values[0], char_to_index, smile_max_length=105))[:1]:\n",
    "    if len(i.shape) > 2:\n",
    "        i = i[0] #for qspr chemvae there is an extra dim\n",
    "    for j in i:\n",
    "#         print(j.shape)\n",
    "        index = sample(j, temperature=0.5)\n",
    "        string += index_to_char[index]\n",
    "string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CC1C(=N)N2CC(=O)CN=C2C2CCCC12O.CC1NCC2COC(COCC#C)OC2C1C\n",
      "CC1NCC2COC(COCC#C)OC2C1C.CC1C(=N)N2CC(=O)CN=C2C2CCCC12O\n",
      "500000\n",
      "Train on 450000 samples, validate on 50000 samples\n",
      "Epoch 1/1\n",
      "450000/450000 [==============================] - 14209s 32ms/step - loss: 4.5817 - acc: 0.6447 - val_loss: 4.4509 - val_acc: 0.6547\n",
      "gen2_2mol_50mil_GDB17_1_1.h5\n",
      "CCNC1C2NC(=N)C1C(N)(C(C)N)C2CO.CC1(OC(OC2=C(O)SC=C12)C(O)CN)C#N\n",
      "CC1(OC(OC2=C(O)SC=C12)C(O)CN)C#N.CCNC1C2NC(=N)C1C(N)(C(C)N)C2CO\n",
      "499999\n",
      "Train on 449999 samples, validate on 50000 samples\n",
      "Epoch 1/1\n",
      "449999/449999 [==============================] - 14190s 32ms/step - loss: 4.4024 - acc: 0.6578 - val_loss: 4.3185 - val_acc: 0.6634\n",
      "gen2_2mol_50mil_GDB17_1_2.h5\n",
      "CC1CNCC(CC(N)C2=CC(N)=C(N)O2)C1.CC1CC2CNCCNCC(CN2)NCCN1\n",
      "CC1CC2CNCCNCC(CN2)NCCN1.CC1CNCC(CC(N)C2=CC(N)=C(N)O2)C1\n",
      "500000\n",
      "Train on 450000 samples, validate on 50000 samples\n",
      "Epoch 1/1\n",
      "450000/450000 [==============================] - 14314s 32ms/step - loss: 4.3197 - acc: 0.6644 - val_loss: 4.6631 - val_acc: 0.6397\n",
      "gen2_2mol_50mil_GDB17_2_1.h5\n",
      "CCCC1(C)CC(CCN=CN1C)C(N)=NC.CC1COC(=O)C=C1C1(O)CC(=O)S(=O)(=O)C1\n",
      "CC1COC(=O)C=C1C1(O)CC(=O)S(=O)(=O)C1.CCCC1(C)CC(CCN=CN1C)C(N)=NC\n",
      "499999\n",
      "Train on 449999 samples, validate on 50000 samples\n",
      "Epoch 1/1\n",
      "449999/449999 [==============================] - 14307s 32ms/step - loss: 4.2733 - acc: 0.6681 - val_loss: 4.3965 - val_acc: 0.6564\n",
      "gen2_2mol_50mil_GDB17_2_2.h5\n",
      "CC1(C)C2CN3CC3C1(N)C1=C2C=NS1.CC(C)C12OCCC(O)(C1C)C1(C)OC21\n",
      "CC(C)C12OCCC(O)(C1C)C1(C)OC21.CC1(C)C2CN3CC3C1(N)C1=C2C=NS1\n",
      "500000\n",
      "Train on 450000 samples, validate on 50000 samples\n",
      "Epoch 1/1\n",
      "450000/450000 [==============================] - 14281s 32ms/step - loss: 4.2432 - acc: 0.6703 - val_loss: 4.2092 - val_acc: 0.6728\n",
      "gen2_2mol_50mil_GDB17_3_1.h5\n",
      "CN1CCC2=C1C(=O)C(N)=C(S2)C(N)CCN.CC1C2C3CC3C3CCC4(N)CC34C12\n",
      "CC1C2C3CC3C3CCC4(N)CC34C12.CN1CCC2=C1C(=O)C(N)=C(S2)C(N)CCN\n",
      "499999\n",
      "Train on 449999 samples, validate on 50000 samples\n",
      "Epoch 1/1\n",
      "449999/449999 [==============================] - 14489s 32ms/step - loss: 4.2300 - acc: 0.6713 - val_loss: 4.2956 - val_acc: 0.6667\n",
      "gen2_2mol_50mil_GDB17_3_2.h5\n",
      "CC1CC2C3CC=C(CC3C(O)C=C12)C=O.CC(O)C1=CC2=C(OCC2)OC(=N)NC=N1\n",
      "CC(O)C1=CC2=C(OCC2)OC(=N)NC=N1.CC1CC2C3CC=C(CC3C(O)C=C12)C=O\n",
      "500000\n",
      "Train on 450000 samples, validate on 50000 samples\n",
      "Epoch 1/1\n",
      "450000/450000 [==============================] - 14506s 32ms/step - loss: 4.2437 - acc: 0.6703 - val_loss: 4.2014 - val_acc: 0.6729\n",
      "gen2_2mol_50mil_GDB17_4_1.h5\n",
      "CC1=NC=C2N=C(C)N=C(C2=C1)C(C)(C)N.CC1CC2CN3C4C2N1C(C)CC34CCN\n",
      "CC1CC2CN3C4C2N1C(C)CC34CCN.CC1=NC=C2N=C(C)N=C(C2=C1)C(C)(C)N\n",
      "499999\n",
      "Train on 449999 samples, validate on 50000 samples\n",
      "Epoch 1/1\n",
      "449999/449999 [==============================] - 14589s 32ms/step - loss: 5.2314 - acc: 0.6067 - val_loss: 4.6834 - val_acc: 0.6447\n",
      "gen2_2mol_50mil_GDB17_4_2.h5\n",
      "CCCC1C(CC(=O)NC1C(C)(N)C#N)NC.CC1NC23C1OCCS(=O)(=O)C2NCC3=NO\n",
      "CC1NC23C1OCCS(=O)(=O)C2NCC3=NO.CCCC1C(CC(=O)NC1C(C)(N)C#N)NC\n",
      "500000\n",
      "Train on 450000 samples, validate on 50000 samples\n",
      "Epoch 1/1\n",
      "450000/450000 [==============================] - 14655s 33ms/step - loss: 4.7716 - acc: 0.6339 - val_loss: 4.3948 - val_acc: 0.6592\n",
      "gen2_2mol_50mil_GDB17_5_1.h5\n",
      "CCNC(=N)C(NCC(C)(C)NC=N)=NCC.CC1(C)CCC2N=C(N1)OC(CCC2O)C#C\n",
      "CC1(C)CCC2N=C(N1)OC(CCC2O)C#C.CCNC(=N)C(NCC(C)(C)NC=N)=NCC\n",
      "499999\n",
      "Train on 449999 samples, validate on 50000 samples\n",
      "Epoch 1/1\n",
      "449999/449999 [==============================] - 14707s 33ms/step - loss: 4.3583 - acc: 0.6621 - val_loss: 4.2995 - val_acc: 0.6673\n",
      "gen2_2mol_50mil_GDB17_5_2.h5\n"
     ]
    }
   ],
   "source": [
    "#ROUND 2: GDB TRAINING BY THE 500K\n",
    "\n",
    "#training array info\n",
    "smile_max_length = 105\n",
    "import json\n",
    "f = open(\"../data/1mil_GDB17.json\",\"r\")\n",
    "char_to_index = json.loads(f.read())\n",
    "\n",
    "#chars needed for 2 mol w/ anions\n",
    "char_to_index['.'] = 33\n",
    "char_to_index['@'] = 34\n",
    "char_to_index['e'] = 35\n",
    "char_to_index['/'] = 36\n",
    "\n",
    "char_set = set(char_to_index.keys())\n",
    "char_list = list(char_to_index.keys())\n",
    "chars_in_dict = len(char_list)\n",
    "\n",
    "#initialize weights\n",
    "chemvae = MoleculeVAE()\n",
    "chemvae.create(char_set, max_length=smile_max_length, weights_file='gen2_2mol_1mil_GDB17_10.h5')\n",
    "\n",
    "for p in range(0,5):\n",
    "    #in this trianing we skip the firt 1Mil GDB since\n",
    "    #we trained on this already in 100K increments, (p+2)\n",
    "    df = pd.read_csv('../data/GDB/GDB17.{}000000'.format(p+2))\n",
    "    df.columns = ['smiles']\n",
    "    data_epoch_size = 500000 #500K\n",
    "    for _epoch in range(2):\n",
    "        selected = df['smiles'][data_epoch_size*_epoch:data_epoch_size*(_epoch+1)].reset_index(drop=True)\n",
    "        values = pd.Series(selected.values + '.' + selected[::-1].values)\n",
    "        print(values[0])\n",
    "        print(values.iloc[-1])\n",
    "        print(len(values))\n",
    "        padded_smiles =  [pad_smiles(i, smile_max_length) for i in values if pad_smiles(i, smile_max_length)]\n",
    "        X_train = np.zeros((len(values), smile_max_length, chars_in_dict), dtype=np.float32)\n",
    "        for i, smile in enumerate(padded_smiles):\n",
    "            for j, char in enumerate(smile):\n",
    "                X_train[i, j, char_to_index[char]] = 1\n",
    "        X_train, X_test = train_test_split(X_train, test_size=0.1, random_state=42) \n",
    "        chemvae.autoencoder.fit(X_train, X_train, shuffle = False, validation_data=(X_test, X_test))\n",
    "        name = 'gen2_2mol_50mil_GDB17_{}_{}.h5'.format(p+1,_epoch+1)\n",
    "        print(name)\n",
    "        chemvae.save(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CC1C(=N)N2CC(=O)CN=C2C2CCCC12O.CC1NCC2COC(COCC#C)OC2C1C\n",
      "CC1NCC2COC(COCC#C)OC2C1C.CC1C(=N)N2CC(=O)CN=C2C2CCCC12O\n",
      "500000\n",
      "salts added: 250003\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-112-4853bfedde82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"salts added: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msalts_added\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0mchemvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'gen2_2mol_50mil_GDB17_num_mix_{}_{}.h5'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_epoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(*arrays, **options)\u001b[0m\n\u001b[1;32m   2057\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2058\u001b[0m     return list(chain.from_iterable((safe_indexing(a, train),\n\u001b[0;32m-> 2059\u001b[0;31m                                      safe_indexing(a, test)) for a in arrays))\n\u001b[0m\u001b[1;32m   2060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2061\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2057\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2058\u001b[0m     return list(chain.from_iterable((safe_indexing(a, train),\n\u001b[0;32m-> 2059\u001b[0;31m                                      safe_indexing(a, test)) for a in arrays))\n\u001b[0m\u001b[1;32m   2060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2061\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36msafe_indexing\u001b[0;34m(X, indices)\u001b[0m\n\u001b[1;32m    158\u001b[0m                                    indices.dtype.kind == 'i'):\n\u001b[1;32m    159\u001b[0m             \u001b[0;31m# This is often substantially faster than X[indices]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#ROUND 3: MIX TRAINING BY THE 500K\n",
    "\n",
    "#training array info\n",
    "smile_max_length = 105\n",
    "import json\n",
    "f = open(\"../data/1mil_GDB17.json\",\"r\")\n",
    "char_to_index = json.loads(f.read())\n",
    "\n",
    "#chars needed for 2 mol w/ anions\n",
    "char_to_index['.'] = 33\n",
    "char_to_index['@'] = 34\n",
    "char_to_index['e'] = 35\n",
    "char_to_index['/'] = 36\n",
    "\n",
    "char_set = set(char_to_index.keys())\n",
    "char_list = list(char_to_index.keys())\n",
    "chars_in_dict = len(char_list)\n",
    "\n",
    "#initialize weights\n",
    "chemvae = MoleculeVAE()\n",
    "chemvae.create(char_set, max_length=smile_max_length, weights_file='gen2_2mol_50mil_GDB17_5_2.h5')\n",
    "\n",
    "for p in range(0,5):\n",
    "    #in this trianing we skip the firt 1Mil GDB since\n",
    "    #we trained on this already in 100K increments, (p+2)\n",
    "    df = pd.read_csv('../data/GDB/GDB17.{}000000'.format(p+2))\n",
    "    df.columns = ['smiles']\n",
    "    data_epoch_size = 500000 #500K\n",
    "    for _epoch in range(2):\n",
    "        selected = df['smiles'][data_epoch_size*_epoch:data_epoch_size*(_epoch+1)].reset_index(drop=True)\n",
    "        values = pd.Series(selected.values + '.' + selected[::-1].values)\n",
    "        print(values[0])\n",
    "        print(values.iloc[-1])\n",
    "        print(len(values))\n",
    "        padded_smiles =  [pad_smiles(i, smile_max_length) for i in values if pad_smiles(i, smile_max_length)]\n",
    "        X_train = np.zeros((len(values), smile_max_length, chars_in_dict), dtype=np.float32)\n",
    "        \n",
    "        salts_added = 0\n",
    "        for i, smile in enumerate(padded_smiles):\n",
    "            #randomly select salt or GDB\n",
    "            linearly_scaled_prob = random.random() < i/data_epoch_size\n",
    "            if linearly_scaled_prob:\n",
    "                salts_added += 1\n",
    "                smile = random.choice(salts)\n",
    "            for j, char in enumerate(smile[:102]):\n",
    "                X_train[i, j, char_to_index[char]] = 1\n",
    "        print(\"salts added: {}\".format(salts_added))\n",
    "\n",
    "        X_train, X_test = train_test_split(X_train, test_size=0.1, random_state=42) \n",
    "        chemvae.autoencoder.fit(X_train, X_train, shuffle = False, validation_data=(X_test, X_test))\n",
    "        name = 'gen2_2mol_50mil_GDB17_num_mix_{}_{}.h5'.format(p+1,_epoch+1)\n",
    "        print(name)\n",
    "        chemvae.save(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROUND 4: SALT TRAINING BY THE 500K\n",
    "\n",
    "#training array info\n",
    "smile_max_length = 105\n",
    "import json\n",
    "f = open(\"../data/1mil_GDB17.json\",\"r\")\n",
    "char_to_index = json.loads(f.read())\n",
    "\n",
    "#chars needed for 2 mol w/ anions\n",
    "char_to_index['.'] = 33\n",
    "char_to_index['@'] = 34\n",
    "char_to_index['e'] = 35\n",
    "char_to_index['/'] = 36\n",
    "\n",
    "char_set = set(char_to_index.keys())\n",
    "char_list = list(char_to_index.keys())\n",
    "chars_in_dict = len(char_list)\n",
    "\n",
    "#initialize weights\n",
    "chemvae = MoleculeVAE()\n",
    "chemvae.create(char_set, max_length=smile_max_length, weights_file='2mol_50mil_GDB17_3.h5')\n",
    "\n",
    "for p in range(0,5):\n",
    "    #in this trianing we skip the firt 1Mil GDB since\n",
    "    #we trained on this already in 100K increments, (p+2)\n",
    "    df = pd.read_csv('../data/GDB/GDB17.{}000000'.format(p+2))\n",
    "    df.columns = ['smiles']\n",
    "    data_epoch_size = 500000 #500K\n",
    "    for _epoch in range(2):\n",
    "        selected = df['smiles'][data_epoch_size*_epoch:data_epoch_size*(_epoch+1)].reset_index(drop=True)\n",
    "        values = pd.Series(selected.values + '.' + selected[::-1].values)\n",
    "        print(values[0])\n",
    "        print(values.iloc[-1])\n",
    "        print(len(values))\n",
    "        padded_smiles =  [pad_smiles(i, smile_max_length) for i in values if pad_smiles(i, smile_max_length)]\n",
    "        X_train = np.zeros((len(values), smile_max_length, chars_in_dict), dtype=np.float32)\n",
    "        \n",
    "        salts_added = 0\n",
    "        for i, smile in enumerate(padded_smiles):\n",
    "            #randomly select salt or GDB\n",
    "#             linearly_scaled_prob = random.random() < i/data_epoch_size\n",
    "#             if linearly_scaled_prob:\n",
    "            salts_added += 1\n",
    "            smile = random.choice(salts)\n",
    "            for j, char in enumerate(smile[:102]):\n",
    "                X_train[i, j, char_to_index[char]] = 1\n",
    "        print(\"salts added: {}\".format(salts_added))\n",
    "\n",
    "        X_train, X_test = train_test_split(X_train, test_size=0.1, random_state=42) \n",
    "        chemvae.autoencoder.fit(X_train, X_train, shuffle = False, validation_data=(X_test, X_test))\n",
    "        name = 'gen2_2mol_50mil_GDB17_num_mix_num_cat_{}_{}.h5'.format(p+1,_epoch+1)\n",
    "        print(name)\n",
    "        chemvae.save(name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
