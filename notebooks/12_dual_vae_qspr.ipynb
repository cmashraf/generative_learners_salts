{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wesleybeckner/anaconda3/envs/py36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import seaborn as sns; sns.set()\n",
    "%matplotlib inline\n",
    "\n",
    "import keras\n",
    "from keras import objectives\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Input, Multiply, Add\n",
    "from keras.optimizers import Adam, Nadam\n",
    "import salty\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from random import shuffle\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "#Keras build\n",
    "from keras import backend as K\n",
    "from keras.objectives import binary_crossentropy #objs or losses\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Lambda, Layer\n",
    "from keras.layers.core import Dense, Activation, Flatten, RepeatVector\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.layers.convolutional import Convolution1D\n",
    "\n",
    "#cation data\n",
    "cations = pd.read_csv('../data/cations.csv')\n",
    "cations = cations['smiles_string']\n",
    "salts = pd.read_csv('../data/salts.csv')\n",
    "salts = salts['smiles_string']\n",
    "categories = pd.read_csv('../data/categories.csv')\n",
    "categories = categories['category']\n",
    "coldic = pd.read_csv('../data/coldic.csv')\n",
    "coldic = coldic.to_dict(orient='records')[0]\n",
    "salt_coldic = pd.read_csv('../data/salt_coldic.csv')\n",
    "salt_coldic = salt_coldic.to_dict(orient='records')[0]\n",
    "salt_categories = pd.read_csv('../data/salt_categories.csv')\n",
    "salt_categories = salt_categories['category']\n",
    "density_coldic = pd.read_csv('../data/density_coldic.csv')\n",
    "density_coldic = density_coldic.to_dict(orient='records')[0]\n",
    "density_categories = pd.read_csv('../data/density_categories.csv')\n",
    "density_categories = density_categories['category']\n",
    "\n",
    "#supporting functions\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "from scripts import *\n",
    "\n",
    "#training array info\n",
    "smile_max_length = 105\n",
    "import json\n",
    "f = open(\"../data/salt_char_to_index.json\",\"r\")\n",
    "char_to_index = json.loads(f.read())\n",
    "char_set = set(char_to_index.keys())\n",
    "char_list = list(char_to_index.keys())\n",
    "chars_in_dict = len(char_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns; sns.set()\n",
    "from matplotlib import colors\n",
    "from itertools import cycle\n",
    "\n",
    "#data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import json\n",
    "import random\n",
    "import copy\n",
    "\n",
    "#ML\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras import objectives\n",
    "from keras.objectives import binary_crossentropy #objs or losses\n",
    "from keras.layers import Dense, Dropout, Input, Multiply, Add, Lambda\n",
    "from keras.layers.core import Dense, Activation, Flatten, RepeatVector\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.layers.convolutional import Convolution1D\n",
    "\n",
    "#chem\n",
    "import salty\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.Fingerprints import FingerprintMols\n",
    "from rdkit import DataStructs\n",
    "from rdkit.Chem import Draw\n",
    "\n",
    "\n",
    "class MoleculeVAE():\n",
    "\n",
    "    autoencoder = None\n",
    "    \n",
    "    def create(self,\n",
    "               charset,\n",
    "               max_length = 105,\n",
    "               latent_rep_size = 292,\n",
    "               weights_file = None,\n",
    "               qspr = False,\n",
    "               conv_layers=3,\n",
    "               gru_layers=3,\n",
    "               conv_filters=9, \n",
    "               conv_kernel_size=9,\n",
    "               gru_output_units=501):\n",
    "        charset_length = len(charset)\n",
    "    \n",
    "        x = Input(shape=(max_length, charset_length))\n",
    "        _, z = self._buildEncoder(x, \n",
    "                                  latent_rep_size, \n",
    "                                  max_length,\n",
    "                                  conv_layers, \n",
    "                                  conv_filters, \n",
    "                                  conv_kernel_size)\n",
    "        self.encoder = Model(x, z)\n",
    "        encoded_input = Input(shape=(latent_rep_size,))\n",
    "        self.decoder = Model(\n",
    "            encoded_input,\n",
    "            self._buildDecoder(\n",
    "                encoded_input,\n",
    "                latent_rep_size,\n",
    "                max_length,\n",
    "                charset_length,\n",
    "                gru_layers,\n",
    "                gru_output_units\n",
    "            )\n",
    "        )\n",
    "\n",
    "        x1 = Input(shape=(max_length, charset_length))\n",
    "        vae_loss, z1 = self._buildEncoder(x1, \n",
    "                                          latent_rep_size, \n",
    "                                          max_length,\n",
    "                                          conv_layers, \n",
    "                                          conv_filters,\n",
    "                                          conv_kernel_size)\n",
    "        \n",
    "        if qspr:\n",
    "            self.autoencoder = Model(\n",
    "                x1,\n",
    "                self._buildDecoderQSPR(\n",
    "                    z1,\n",
    "                    latent_rep_size,\n",
    "                    max_length,\n",
    "                    charset_length,\n",
    "                    gru_layers,\n",
    "                    gru_output_units\n",
    "                )\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            self.autoencoder = Model(\n",
    "                x1,\n",
    "                self._buildDecoder(\n",
    "                    z1,\n",
    "                    latent_rep_size,\n",
    "                    max_length,\n",
    "                    charset_length,\n",
    "                    gru_layers,\n",
    "                    gru_output_units\n",
    "                )\n",
    "            )\n",
    "            \n",
    "        self.qspr = Model(\n",
    "            x1,\n",
    "            self._buildQSPR(\n",
    "                z1,\n",
    "                latent_rep_size,\n",
    "                max_length,\n",
    "                charset_length\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "        if weights_file:\n",
    "            self.autoencoder.load_weights(weights_file, by_name = True)\n",
    "            self.encoder.load_weights(weights_file, by_name = True)\n",
    "            self.decoder.load_weights(weights_file, by_name = True)\n",
    "            self.qspr.load_weights(weights_file, by_name = True)\n",
    "        if qspr:\n",
    "            self.autoencoder.compile(optimizer = 'Adam',\n",
    "                                     loss = {'decoded_mean': vae_loss, 'qspr': 'mean_squared_error'},\n",
    "                                     metrics = ['accuracy', 'mse'])\n",
    "        else:\n",
    "            self.autoencoder.compile(optimizer = 'Adam',\n",
    "                                     loss = {'decoded_mean': vae_loss},\n",
    "                                     metrics = ['accuracy'])\n",
    "    def _buildEncoder(self, x, latent_rep_size, max_length, epsilon_std = 0.01, conv_layers=3, conv_filters=9, conv_kernel_size=9):\n",
    "        h = Convolution1D(9, 9, activation = 'relu', name='conv_1')(x)\n",
    "        for convolution in range(conv_layers-2):\n",
    "            h = Convolution1D(conv_filters, conv_kernel_size, activation = 'relu', name='conv_{}'.format(convolution+2))(h)\n",
    "        h = Convolution1D(10, 11, activation = 'relu', name='conv_{}'.format(conv_layers))(h)\n",
    "        h = Flatten(name='flatten_1')(h)\n",
    "        h = Dense(435, activation = 'relu', name='dense_1')(h)\n",
    "\n",
    "        def sampling(args):\n",
    "            z_mean_, z_log_var_ = args\n",
    "            batch_size = K.shape(z_mean_)[0]\n",
    "            epsilon = K.random_normal(shape=(batch_size, latent_rep_size), mean=0., stddev = epsilon_std)\n",
    "            return z_mean_ + K.exp(z_log_var_ / 2) * epsilon\n",
    "\n",
    "        z_mean = Dense(latent_rep_size, name='z_mean', activation = 'linear')(h)\n",
    "        z_log_var = Dense(latent_rep_size, name='z_log_var', activation = 'linear')(h)\n",
    "\n",
    "        def vae_loss(x, x_decoded_mean):\n",
    "            x = K.flatten(x)\n",
    "            x_decoded_mean = K.flatten(x_decoded_mean)\n",
    "            xent_loss = max_length * objectives.binary_crossentropy(x, x_decoded_mean)\n",
    "            kl_loss = - 0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis = -1)\n",
    "            return xent_loss + kl_loss\n",
    "\n",
    "        return (vae_loss, Lambda(sampling, output_shape=(latent_rep_size,), name='lambda')([z_mean, z_log_var]))\n",
    "\n",
    "    def _buildDecoderQSPR(self, z, latent_rep_size, max_length, charset_length, gru_layers=3):\n",
    "\n",
    "        h = Dense(latent_rep_size, name='latent_input', activation = 'relu')(z)\n",
    "        h = RepeatVector(max_length, name='repeat_vector')(h)\n",
    "        h = GRU(501, return_sequences = True, name='gru_1')(h)\n",
    "        for gru in range(gru_layers-2):\n",
    "            h = GRU(501, return_sequences = True, name='gru_{}'.format(gru+2))(h)\n",
    "        h = GRU(501, return_sequences = True, name='gru_{}'.format(gru_layers))(h)\n",
    "        smiles_decoded = TimeDistributed(Dense(charset_length, activation='softmax'), name='decoded_mean')(h)\n",
    "\n",
    "        h = Dense(latent_rep_size, name='qspr_input', activation='relu')(z)\n",
    "        h = Dense(100, activation='relu', name='hl_1')(h)\n",
    "        h = Dropout(0.5)(h)\n",
    "        smiles_qspr = Dense(1, activation='linear', name='qspr')(h)\n",
    "\n",
    "        return smiles_decoded, smiles_qspr\n",
    "\n",
    "    def _buildDecoder(self, z, latent_rep_size, max_length, charset_length, gru_layers=3, gru_output_units=501):\n",
    "\n",
    "        h = Dense(latent_rep_size, name='latent_input', activation = 'relu')(z)\n",
    "        h = RepeatVector(max_length, name='repeat_vector')(h)\n",
    "        h = GRU(501, return_sequences = True, name='gru_1')(h)\n",
    "        for gru in range(gru_layers-2):\n",
    "            h = GRU(gru_output_units, return_sequences = True, name='gru_{}'.format(gru+2))(h)\n",
    "        h = GRU(501, return_sequences = True, name='gru_{}'.format(gru_layers))(h)\n",
    "        smiles_decoded = TimeDistributed(Dense(charset_length, activation='softmax'), name='decoded_mean')(h)\n",
    "\n",
    "        return smiles_decoded\n",
    "    \n",
    "    def _buildQSPR(self, z, latent_rep_size, max_length, charset_length):\n",
    "        h = Dense(latent_rep_size, name='latent_input', activation='relu')(z)\n",
    "        h = Dense(100, activation='relu', name='hl_1')(h)\n",
    "        h = Dropout(0.5)(h)\n",
    "        return Dense(1, activation='linear', name='qspr')(h)\n",
    "\n",
    "    def save(self, filename):\n",
    "        self.autoencoder.save_weights(filename)\n",
    "    \n",
    "    def load(self, charset, weights_file, latent_rep_size = 292):\n",
    "        self.create(charset, weights_file = weights_file, latent_rep_size = latent_rep_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcharset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m105\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_rep_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m292\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqspr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m <no docstring>\n",
       "\u001b[0;31mFile:\u001b[0m      /media/wesleybeckner/weshhd/wes/Dropbox/Python/py3/generative_learners_salts/scripts/vae.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vae = MoleculeVAE()\n",
    "vae.create?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training array info\n",
    "import json\n",
    "f = open(\"../data/gdb_char_to_index.json\",\"r\")\n",
    "char_to_index = json.loads(f.read())\n",
    "char_set = set(char_to_index.keys())\n",
    "char_list = list(char_to_index.keys())\n",
    "chars_in_dict = len(char_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.create(char_set, \n",
    "           max_length=51, \n",
    "           latent_rep_size=292,\n",
    "           weights_file='1mil_GDB17_10.h5',\n",
    "           qspr=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For loop to save semi-trained model to\n",
    "#view PCAs and Z distributions during training\n",
    "\n",
    "#training array info\n",
    "smile_max_length = 51\n",
    "import json\n",
    "f = open(\"../data/1mil_GDB17.json\",\"r\")\n",
    "char_to_index = json.loads(f.read())\n",
    "char_set = set(char_to_index.keys())\n",
    "char_list = list(char_to_index.keys())\n",
    "chars_in_dict = len(char_list)\n",
    "\n",
    "#training data\n",
    "chemvae = MoleculeVAE()\n",
    "char_set = set(char_to_index.keys())\n",
    "weights_file = '1mil_GDB17_10.h5'\n",
    "chemvae.load(char_set, weights_file)\n",
    "# df = pd.read_csv('../data/GDB17.1000000', names=['smiles'])\n",
    "data_size = 100000\n",
    "\n",
    "\n",
    "for p in range(0,5):\n",
    "#     values = df['smiles'][data_size*p:data_size*(p+1)]\n",
    "#     padded_smiles =  [pad_smiles(i, smile_max_length) for i in values if pad_smiles(i, smile_max_length)]\n",
    "    X_train = np.zeros((data_size, smile_max_length, chars_in_dict), dtype=np.float32)\n",
    "    \n",
    "    # for each i, randomly select whether to sample from GDB or cations (padded_smiles_2)\n",
    "#     for i, smile in enumerate(padded_smiles[:data_size]):\n",
    "    for i in range(data_size):\n",
    "#         linearly_scaled_prob = random.random() < 0.5#i/data_size\n",
    "#         if linearly_scaled_prob:\n",
    "#             smile = random.choice(cations)\n",
    "        smile = random.choice(cations)\n",
    "        for j, char in enumerate(smile):\n",
    "            X_train[i, j, char_to_index[char]] = 1\n",
    "\n",
    "    X_train, X_test = train_test_split(X_train, test_size=0.33, random_state=42)   \n",
    "    chemvae.autoencoder.fit(X_train, X_train, shuffle = False, validation_data=(X_test, X_test))\n",
    "    chemvae.save('1mil_GDB17_500K_mix_500K_cation_{}.h5'.format(p+1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1mil_GDB17_10.h5\n",
      "1mil_GDB17_1.h5\n",
      "1mil_GDB17_2.h5\n",
      "1mil_GDB17_3.h5\n",
      "1mil_GDB17_4.h5\n",
      "1mil_GDB17_500K_mix_1.h5\n",
      "1mil_GDB17_500K_mix_2.h5\n",
      "1mil_GDB17_500K_mix_3.h5\n",
      "1mil_GDB17_500K_mix_4.h5\n",
      "1mil_GDB17_500K_mix_500K_cation_1.h5\n",
      "1mil_GDB17_500K_mix_500K_cation_2.h5\n",
      "1mil_GDB17_500K_mix_500K_cation_3.h5\n",
      "1mil_GDB17_500K_mix_500K_cation_4.h5\n",
      "1mil_GDB17_500K_mix_500K_cation_5.h5\n",
      "1mil_GDB17_500K_mix_5.h5\n",
      "1mil_GDB17_5.h5\n",
      "1mil_GDB17_6.h5\n",
      "1mil_GDB17_7.h5\n",
      "1mil_GDB17_8.h5\n",
      "1mil_GDB17_9.h5\n",
      "2mol_1mil_GDB17_10.h5\n",
      "2mol_1mil_GDB17_1.h5\n",
      "2mol_1mil_GDB17_2.h5\n",
      "2mol_1mil_GDB17_3.h5\n",
      "2mol_1mil_GDB17_4.h5\n",
      "2mol_1mil_GDB17_5.h5\n",
      "2mol_1mil_GDB17_6.h5\n",
      "2mol_1mil_GDB17_7.h5\n",
      "2mol_1mil_GDB17_8.h5\n",
      "2mol_1mil_GDB17_9.h5\n",
      "2mol_50mil_GDB17_2.h5\n",
      "2mol_50mil_GDB17_3_1mil_mix_1.h5\n",
      "2mol_50mil_GDB17_3_1mil_mix_1mil_cat_1.h5\n",
      "2mol_50mil_GDB17_3.h5\n",
      "gen2_2mol_1mil_GDB17_10.h5\n",
      "gen2_2mol_1mil_GDB17_1.h5\n",
      "gen2_2mol_1mil_GDB17_2.h5\n",
      "gen2_2mol_1mil_GDB17_3.h5\n",
      "gen2_2mol_1mil_GDB17_4.h5\n",
      "gen2_2mol_1mil_GDB17_5.h5\n",
      "gen2_2mol_1mil_GDB17_6.h5\n",
      "gen2_2mol_1mil_GDB17_7.h5\n",
      "gen2_2mol_1mil_GDB17_8.h5\n",
      "gen2_2mol_1mil_GDB17_9.h5\n",
      "gen2_2mol_1mil_GDB17_latent_embeddings.npy\n",
      "gen2_2mol_50mil_GDB17_1_1.h5\n",
      "gen2_2mol_50mil_GDB17_1_2.h5\n",
      "gen2_2mol_50mil_GDB17_2_1.h5\n",
      "gen2_2mol_50mil_GDB17_2_2.h5\n",
      "gen2_2mol_50mil_GDB17_3_1.h5\n",
      "gen2_2mol_50mil_GDB17_3_2.h5\n",
      "gen2_2mol_50mil_GDB17_4_1.h5\n",
      "gen2_2mol_50mil_GDB17_4_2.h5\n",
      "gen2_2mol_50mil_GDB17_5_1.h5\n",
      "gen2_2mol_50mil_GDB17_5_2_cpt_vae_100_epochs.h5\n",
      "gen2_2mol_50mil_GDB17_5_2_cpt_vae_10_epochs.h5\n",
      "gen2_2mol_50mil_GDB17_5_2_cpt_vae_1_epochs.h5\n",
      "gen2_2mol_50mil_GDB17_5_2_density_vae_100_epochs.h5\n",
      "gen2_2mol_50mil_GDB17_5_2_density_vae_10_epochs.h5\n",
      "gen2_2mol_50mil_GDB17_5_2_density_vae_1_epochs.h5\n",
      "gen2_2mol_50mil_GDB17_5_2.h5\n",
      "gen2_2mol_50mil_GDB17_5_2_melting_point_vae_100_epochs.h5\n",
      "gen2_2mol_50mil_GDB17_5_2_melting_point_vae_10_epochs.h5\n",
      "gen2_2mol_50mil_GDB17_5_2_melting_point_vae_1_epochs.h5\n",
      "gen2_2mol_50mil_GDB17_5_2_mix_1_1.h5\n",
      "gen2_2mol_50mil_GDB17_5_2_mix_1_2.h5\n",
      "gen2_2mol_50mil_GDB17_5_2_mix_2_1.h5\n",
      "gen2_2mol_50mil_GDB17_5_2_mix_2_2.h5\n",
      "gen2_2mol_50mil_GDB17_5_2_mix_3_1.h5\n",
      "gen2_2mol_50mil_GDB17_5_2_mix_3_2.h5\n",
      "gen2_2mol_50mil_GDB17_5_2_mix_4_1.h5\n",
      "gen2_2mol_50mil_GDB17_5_2_mix_4_2.h5\n",
      "gen2_2mol_50mil_GDB17_5_2_mix_5_1.h5\n",
      "gen2_2mol_50mil_GDB17_5_2_mix_5_2_cat_1_1.h5\n",
      "gen2_2mol_50mil_GDB17_5_2_mix_5_2_cat_1_2.h5\n",
      "gen2_2mol_50mil_GDB17_5_2_mix_5_2_cat_2_1.h5\n",
      "gen2_2mol_50mil_GDB17_5_2_mix_5_2_cat_2_2.h5\n",
      "gen2_2mol_50mil_GDB17_5_2_mix_5_2_cat_3_1.h5\n",
      "gen2_2mol_50mil_GDB17_5_2_mix_5_2_cat_3_2.h5\n",
      "gen2_2mol_50mil_GDB17_5_2_mix_5_2_cat_4_1.h5\n",
      "gen2_2mol_50mil_GDB17_5_2_mix_5_2_cat_4_2.h5\n",
      "gen2_2mol_50mil_GDB17_5_2_mix_5_2_cat_5_1.h5\n",
      "gen2_2mol_50mil_GDB17_5_2_mix_5_2_cat_5_2_cpt_vae_100_epochs.h5\n",
      "gen2_2mol_50mil_GDB17_5_2_mix_5_2_cat_5_2_cpt_vae_10_epochs.h5\n",
      "gen2_2mol_50mil_GDB17_5_2_mix_5_2_cat_5_2_cpt_vae_1_epochs.h5\n",
      "gen2_2mol_50mil_GDB17_5_2_mix_5_2_cat_5_2_density_vae_100_epochs.h5\n",
      "gen2_2mol_50mil_GDB17_5_2_mix_5_2_cat_5_2_density_vae_10_epochs.h5\n",
      "gen2_2mol_50mil_GDB17_5_2_mix_5_2_cat_5_2_density_vae_1_epochs.h5\n",
      "gen2_2mol_50mil_GDB17_5_2_mix_5_2_cat_5_2.h5\n",
      "gen2_2mol_50mil_GDB17_5_2_mix_5_2_cat_5_2_melting_point_vae_100_epochs.h5\n",
      "gen2_2mol_50mil_GDB17_5_2_mix_5_2_cat_5_2_melting_point_vae_10_epochs.h5\n",
      "gen2_2mol_50mil_GDB17_5_2_mix_5_2_cat_5_2_melting_point_vae_1_epochs.h5\n",
      "gen2_2mol_50mil_GDB17_5_2_mix_5_2_cat_5_2_thermal_conductivity_vae_100_epochs.h5\n",
      "gen2_2mol_50mil_GDB17_5_2_mix_5_2_cat_5_2_thermal_conductivity_vae_10_epochs.h5\n",
      "gen2_2mol_50mil_GDB17_5_2_mix_5_2_cat_5_2_thermal_conductivity_vae_1_epochs.h5\n",
      "gen2_2mol_50mil_GDB17_5_2_mix_5_2_cat_5_2_viscosity_vae_100_epochs.h5\n",
      "gen2_2mol_50mil_GDB17_5_2_mix_5_2_cat_5_2_viscosity_vae_10_epochs.h5\n",
      "gen2_2mol_50mil_GDB17_5_2_mix_5_2_cat_5_2_viscosity_vae_1_epochs.h5\n",
      "gen2_2mol_50mil_GDB17_5_2_mix_5_2_cpt_vae_100_epochs.h5\n",
      "gen2_2mol_50mil_GDB17_5_2_mix_5_2_cpt_vae_10_epochs.h5\n",
      "gen2_2mol_50mil_GDB17_5_2_mix_5_2_cpt_vae_1_epochs.h5\n",
      "gen2_2mol_50mil_GDB17_5_2_mix_5_2_density_vae_100_epochs.h5\n",
      "gen2_2mol_50mil_GDB17_5_2_mix_5_2_density_vae_10_epochs.h5\n",
      "gen2_2mol_50mil_GDB17_5_2_mix_5_2_density_vae_1_epochs.h5\n",
      "gen2_2mol_50mil_GDB17_5_2_mix_5_2.h5\n",
      "gen2_2mol_50mil_GDB17_5_2_mix_5_2_melting_point_vae_100_epochs.h5\n",
      "gen2_2mol_50mil_GDB17_5_2_mix_5_2_melting_point_vae_10_epochs.h5\n",
      "gen2_2mol_50mil_GDB17_5_2_mix_5_2_melting_point_vae_1_epochs.h5\n",
      "gen2_2mol_50mil_GDB17_5_2_mix_5_2_thermal_conductivity_vae_100_epochs.h5\n",
      "gen2_2mol_50mil_GDB17_5_2_mix_5_2_thermal_conductivity_vae_10_epochs.h5\n",
      "gen2_2mol_50mil_GDB17_5_2_mix_5_2_thermal_conductivity_vae_1_epochs.h5\n",
      "gen2_2mol_50mil_GDB17_5_2_mix_5_2_viscosity_vae_100_epochs.h5\n",
      "gen2_2mol_50mil_GDB17_5_2_mix_5_2_viscosity_vae_10_epochs.h5\n",
      "gen2_2mol_50mil_GDB17_5_2_mix_5_2_viscosity_vae_1_epochs.h5\n",
      "gen2_2mol_50mil_GDB17_5_2_qspr_density_ATP_30_epoch.h5\n",
      "gen2_2mol_50mil_GDB17_5_2_thermal_conductivity_vae_100_epochs.h5\n",
      "gen2_2mol_50mil_GDB17_5_2_thermal_conductivity_vae_10_epochs.h5\n",
      "gen2_2mol_50mil_GDB17_5_2_thermal_conductivity_vae_1_epochs.h5\n",
      "gen2_2mol_50mil_GDB17_5_2_thermal_conductivity_vae_2_epochs.h5\n",
      "gen2_2mol_50mil_GDB17_5_2_viscosity_vae_100_epochs.h5\n",
      "gen2_2mol_50mil_GDB17_5_2_viscosity_vae_10_epochs.h5\n",
      "gen2_2mol_50mil_GDB17_5_2_viscosity_vae_1_epochs.h5\n",
      "gen2_2mol_6mil_GDB17_latent_embeddings.npy\n",
      "gen2_2mol_6mil_GDB17_mix_latent_embeddings.npy\n",
      "gen2_2mol_6mil_GDB17_mix_latent_embeddings_with_salts.npy\n",
      "gen2_2mol_6mil_GDB17_mix_salt_latent_embeddings.npy\n",
      "history_gen2_2mol_50mil_GDB17_5_2_cpt_vae_100_epochs.json\n",
      "history_gen2_2mol_50mil_GDB17_5_2_density_vae_100_epochs.json\n",
      "history_gen2_2mol_50mil_GDB17_5_2_melting_point_vae_100_epochs.json\n",
      "history_gen2_2mol_50mil_GDB17_5_2_mix_5_2_cat_1_1.h5.json\n",
      "history_gen2_2mol_50mil_GDB17_5_2_mix_5_2_cat_1_2.h5.json\n",
      "history_gen2_2mol_50mil_GDB17_5_2_mix_5_2_cat_2_1.h5.json\n",
      "history_gen2_2mol_50mil_GDB17_5_2_mix_5_2_cat_2_2.h5.json\n",
      "history_gen2_2mol_50mil_GDB17_5_2_mix_5_2_cat_3_1.h5.json\n",
      "history_gen2_2mol_50mil_GDB17_5_2_mix_5_2_cat_3_2.h5.json\n",
      "history_gen2_2mol_50mil_GDB17_5_2_mix_5_2_cat_4_1.h5.json\n",
      "history_gen2_2mol_50mil_GDB17_5_2_mix_5_2_cat_4_2.h5.json\n",
      "history_gen2_2mol_50mil_GDB17_5_2_mix_5_2_cat_5_1.h5.json\n",
      "history_gen2_2mol_50mil_GDB17_5_2_mix_5_2_cat_5_2_cpt_vae_100_epochs.json\n",
      "history_gen2_2mol_50mil_GDB17_5_2_mix_5_2_cat_5_2_cpt_vae_10_epochs.json\n",
      "history_gen2_2mol_50mil_GDB17_5_2_mix_5_2_cat_5_2_cpt_vae_1_epochs.json\n",
      "history_gen2_2mol_50mil_GDB17_5_2_mix_5_2_cat_5_2_density_vae_100_epochs.json\n",
      "history_gen2_2mol_50mil_GDB17_5_2_mix_5_2_cat_5_2_density_vae_10_epochs.json\n",
      "history_gen2_2mol_50mil_GDB17_5_2_mix_5_2_cat_5_2_density_vae_1_epochs.json\n",
      "history_gen2_2mol_50mil_GDB17_5_2_mix_5_2_cat_5_2.h5.json\n",
      "history_gen2_2mol_50mil_GDB17_5_2_mix_5_2_cat_5_2_melting_point_vae_100_epochs.json\n",
      "history_gen2_2mol_50mil_GDB17_5_2_mix_5_2_cat_5_2_melting_point_vae_10_epochs.json\n",
      "history_gen2_2mol_50mil_GDB17_5_2_mix_5_2_cat_5_2_melting_point_vae_1_epochs.json\n",
      "history_gen2_2mol_50mil_GDB17_5_2_mix_5_2_cat_5_2_thermal_conductivity_vae_100_epochs.json\n",
      "history_gen2_2mol_50mil_GDB17_5_2_mix_5_2_cat_5_2_thermal_conductivity_vae_10_epochs.json\n",
      "history_gen2_2mol_50mil_GDB17_5_2_mix_5_2_cat_5_2_thermal_conductivity_vae_1_epochs.json\n",
      "history_gen2_2mol_50mil_GDB17_5_2_mix_5_2_cat_5_2_viscosity_vae_100_epochs.json\n",
      "history_gen2_2mol_50mil_GDB17_5_2_mix_5_2_cat_5_2_viscosity_vae_10_epochs.json\n",
      "history_gen2_2mol_50mil_GDB17_5_2_mix_5_2_cat_5_2_viscosity_vae_1_epochs.json\n",
      "history_gen2_2mol_50mil_GDB17_5_2_mix_5_2_cpt_vae_100_epochs.json\n",
      "history_gen2_2mol_50mil_GDB17_5_2_mix_5_2_cpt_vae_10_epochs.json\n",
      "history_gen2_2mol_50mil_GDB17_5_2_mix_5_2_cpt_vae_1_epochs.json\n",
      "history_gen2_2mol_50mil_GDB17_5_2_mix_5_2_density_vae_100_epochs.json\n",
      "history_gen2_2mol_50mil_GDB17_5_2_mix_5_2_density_vae_10_epochs.json\n",
      "history_gen2_2mol_50mil_GDB17_5_2_mix_5_2_density_vae_1_epochs.json\n",
      "history_gen2_2mol_50mil_GDB17_5_2_mix_5_2_melting_point_vae_100_epochs.json\n",
      "history_gen2_2mol_50mil_GDB17_5_2_mix_5_2_melting_point_vae_10_epochs.json\n",
      "history_gen2_2mol_50mil_GDB17_5_2_mix_5_2_melting_point_vae_1_epochs.json\n",
      "history_gen2_2mol_50mil_GDB17_5_2_mix_5_2_thermal_conductivity_vae_100_epochs.json\n",
      "history_gen2_2mol_50mil_GDB17_5_2_mix_5_2_thermal_conductivity_vae_10_epochs.json\n",
      "history_gen2_2mol_50mil_GDB17_5_2_mix_5_2_thermal_conductivity_vae_1_epochs.json\n",
      "history_gen2_2mol_50mil_GDB17_5_2_mix_5_2_viscosity_vae_100_epochs.json\n",
      "history_gen2_2mol_50mil_GDB17_5_2_mix_5_2_viscosity_vae_10_epochs.json\n",
      "history_gen2_2mol_50mil_GDB17_5_2_mix_5_2_viscosity_vae_1_epochs.json\n",
      "history_gen2_2mol_50mil_GDB17_5_2_thermal_conductivity_vae_100_epochs.json\n",
      "history_gen2_2mol_50mil_GDB17_5_2_thermal_conductivity_vae_1_epochs.json\n",
      "history_gen2_2mol_50mil_GDB17_5_2_thermal_conductivity_vae_2_epochs.json\n",
      "history_gen2_2mol_50mil_GDB17_5_2_viscosity_vae_100_epochs.json\n"
     ]
    }
   ],
   "source": [
    "ls *GDB*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.autoencoder.layers[1].name = 'conv_wes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'conv_wes'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae.autoencoder.layers[1].name"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
